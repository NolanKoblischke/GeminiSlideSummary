[
    {
        "video_id": "Ht9PogeTT54",
        "transcript": [
            {
                "timestamp": "00:00",
                "text": "Joe here, everyone."
            },
            {
                "timestamp": "00:02",
                "text": "So, uh, this should be a very interesting talk today."
            },
            {
                "timestamp": "00:05",
                "text": "Uh, Joe's coming to us from Canberra, Australia where she's at the Australian National University."
            },
            {
                "timestamp": "00:10",
                "text": "And as you may have noticed her subject field is not traditionally speaking in language technology but it's actually astronomy."
            },
            {
                "timestamp": "00:16",
                "text": "Um, and one of the things I'm most excited about is the fact that nowadays people outside of just CLSP are caring about language technology."
            },
            {
                "timestamp": "00:22",
                "text": "So we're getting all kinds of astronomers and others that are really interested in our sort of work."
            },
            {
                "timestamp": "00:26",
                "text": "So, there's some cool interdisciplinary collaborations going on here."
            },
            {
                "timestamp": "00:30",
                "text": "Um, I'm really excited for her talk."
            },
            {
                "timestamp": "00:32",
                "text": "The first thing that of course that jumped out to me was uh, AstroLLaMA."
            },
            {
                "timestamp": "00:35",
                "text": "And if you're thinking that AstroLLaMA is probably just applying LaMA B2 to astronomy data, you're actually wrong."
            },
            {
                "timestamp": "00:40",
                "text": "But no, no, it's actually correct."
            },
            {
                "timestamp": "00:42",
                "text": "So, um, yeah, but uh, I'm really excited to hear her talk about LaMA, the adventure in space."
            },
            {
                "timestamp": "00:46",
                "text": "Oh, yeah, I don't think that's what the picture is on the paper."
            },
            {
                "timestamp": "00:49",
                "text": "But yeah, so excited for Joe's talk."
            },
            {
                "timestamp": "00:50",
                "text": "So please welcome Joe."
            },
            {
                "timestamp": "00:52",
                "text": "Oh, thank you so much."
            },
            {
                "timestamp": "00:56",
                "text": "Thank you so much Kenton, and thank you really everyone for having me here."
            },
            {
                "timestamp": "00:59",
                "text": "I'd also like to thank John for having me here, and George, and I'm super excited to learn about everyone and your research."
            },
            {
                "timestamp": "01:06",
                "text": "So, today I'll talk a bit about um, consortium that I am a part of."
            },
            {
                "timestamp": "01:10",
                "text": "We are called UniverseTBD, and we stand for a universe to be decided."
            },
            {
                "timestamp": "01:15",
                "text": "When we chose the name, we thought about it quite deeply, it was like, why go for something certain, when the world around us is quite uncertain?"
            },
            {
                "timestamp": "01:21",
                "text": "So, we adapt as things change around us."
            },
            {
                "timestamp": "01:25",
                "text": "Um, specifically, today I'll talk to you about our research with foundation models for astronomy."
            },
            {
                "timestamp": "01:34",
                "text": "Sorry, there's, okay."
            },
            {
                "timestamp": "01:37",
                "text": "But, before I start I would like to start with this slide, uh, Coelorum perrupit claustra. This is written on the epitaph of Sir William Herschel."
            },
            {
                "timestamp": "01:46",
                "text": "These words mean that he broke through the barriers of heaven, and in my life I've had two moments when I thought that I almost did the same."
            },
            {
                "timestamp": "01:55",
                "text": "The first time was when I was 6 years old and I saw uh, a solar eclipse reflected into a bucket of water back home in Romania."
            },
            {
                "timestamp": "02:01",
                "text": "Where we didn't have much, and there were many people around me told me that I'll never be an astronomer."
            },
            {
                "timestamp": "02:08",
                "text": "And then yet, I'm here with you today. So in a sense, where there is a tool, and there is a way we can make things happen."
            },
            {
                "timestamp": "02:15",
                "text": "The second time was when Katie here and I were sitting at the Flatiron Institute, almost 8 months ago, together and thinking, how can we use this new technology in large language models and foundation models to truly open up our field to the world?"
            },
            {
                "timestamp": "02:29",
                "text": "So, that's how UniverseTBD came to be."
            },
            {
                "timestamp": "02:33",
                "text": "Um, we are pretty much an AI research lab, I like to call us an AI research hack lab, most of the time, but my more ethically concerned uh, colleagues would like to go for this denomination."
            },
            {
                "timestamp": "02:44",
                "text": "We have 30+ people working together uh, to to democratize astronomy for everyone, and science for everyone."
            },
            {
                "timestamp": "02:52",
                "text": "We are a bunch of people coming from all walks of life."
            },
            {
                "timestamp": "02:55",
                "text": "We have astronomers, software engineers, computer scientists, and NLP experts, and we are growing."
            },
            {
                "timestamp": "03:01",
                "text": "So we are very excited to present what we've been working on today to you as well."
            },
            {
                "timestamp": "03:06",
                "text": "And then we also have four continents and counting."
            },
            {
                "timestamp": "03:09",
                "text": "So, we have people from everywhere."
            },
            {
                "timestamp": "03:11",
                "text": "We have Europe, India, China, America."
            },
            {
                "timestamp": "03:16",
                "text": "And it's a pain to set meetings up, but we make it happen. So, I hope that says how determined we are to make this become reality."
            },
            {
                "timestamp": "03:25",
                "text": "So now you might ask yourself, what dream are we actually talking about?"
            },
            {
                "timestamp": "03:29",
                "text": "So our dream is and is sort of, uh, reflected into our mission."
            },
            {
                "timestamp": "03:33",
                "text": "So our mission is to democratize science for everyone."
            },
            {
                "timestamp": "03:37",
                "text": "And what do I mean when I say democratization?"
            },
            {
                "timestamp": "03:39",
                "text": "I mean the problem that we face right now in our fields, and in our research and in our society to some extent."
            },
            {
                "timestamp": "03:46",
                "text": "So, there is a monopolization of scientific resources that reflect in an opportunity gap around institutions."
            },
            {
                "timestamp": "03:55",
                "text": "Many people that come from smaller institutions, also in countries without the access that we have here or in Europe or in Australia, don't have the same access to resources that we do have."
            },
            {
                "timestamp": "04:06",
                "text": "So what can we do to help them?"
            },
            {
                "timestamp": "04:09",
                "text": "It is very likely that as technology advances this gap will likely increase and I would like to be proven wrong."
            },
            {
                "timestamp": "04:15",
                "text": "But until then, this is a problem that we have to think about deeply."
            },
            {
                "timestamp": "04:19",
                "text": "That what can we do to actually stand up and try to do something that uh, will change this problem?"
            },
            {
                "timestamp": "04:27",
                "text": "So, to get to that point uh, we started with a small goal, to build a little uh, model, the AstroLLaMA."
            },
            {
                "timestamp": "04:35",
                "text": "But our goal much, we have a larger goal in mind."
            },
            {
                "timestamp": "04:38",
                "text": "We want to build semi-autonomous AI systems capable to um, aid researchers everywhere."
            },
            {
                "timestamp": "04:44",
                "text": "So, we do not want to replace human beings."
            },
            {
                "timestamp": "04:47",
                "text": "We want to empower human beings to do the best research they have, and we also want to empower enthusiasts to engage with our research in many meaningful ways."
            },
            {
                "timestamp": "04:56",
                "text": "You might think about it why start with astronomy?"
            },
            {
                "timestamp": "05:00",
                "text": "Because we are astronomers, it's the easiest thing we can do."
            },
            {
                "timestamp": "05:03",
                "text": "But, also because astronomy is a quite unique field in that we actually work with data sets where we can explore and experiment and not afraid to be sued a lot."
            },
            {
                "timestamp": "05:13",
                "text": "So for example, astronomy has the simplified privacy management structure, where privacy is not quite the same problem as it would be in medicine for example."
            },
            {
                "timestamp": "05:24",
                "text": "It is broad appeal."
            },
            {
                "timestamp": "05:26",
                "text": "Astronomy is the oldest science on the planet."
            },
            {
                "timestamp": "05:29",
                "text": "Uh, we've done this for many millennia, and people everywhere want to look up and understand what's happening with our universe. So, we can appeal to a broad audience."
            },
            {
                "timestamp": "05:38",
                "text": "There is a rich, astronomy is a rich resource for innovation."
            },
            {
                "timestamp": "05:42",
                "text": "We have a lot of data."
            },
            {
                "timestamp": "05:44",
                "text": "We have data from space, we have data from ground, and these data objects are actually quite unique."
            },
            {
                "timestamp": "05:50",
                "text": "We have images, as you see today in my talk, we have texts, but we also have time series, stellar spectra, and so on."
            },
            {
                "timestamp": "05:59",
                "text": "And finally, and the reason why I'm happy to be here with you today is that we are a very, very large community."
            },
            {
                "timestamp": "06:06",
                "text": "And right now we are a community that wants to jump in and connect with other communities, especially in the computer science field."
            },
            {
                "timestamp": "06:14",
                "text": "Um, we want to keep growing and build interdisciplinary solutions to the problem that I talked to you about."
            },
            {
                "timestamp": "06:21",
                "text": "So, where do we start? We start with natural language processing."
            },
            {
                "timestamp": "06:25",
                "text": "Because that's what inspired this work, when ChatGPT came to life I was very, very particularly impressed with that uh, solution."
            },
            {
                "timestamp": "06:33",
                "text": "Not so sure at the moment, but then I was very impressed."
            },
            {
                "timestamp": "06:36",
                "text": "So, um, as part of UniverseTBD we have to, we have two tracks."
            },
            {
                "timestamp": "06:40",
                "text": "We have the research track uh, and as part of the research track we have developed already AstroLLaMA-1 model."
            },
            {
                "timestamp": "06:48",
                "text": "And I'll show you a few more details very soon, but we also have quite a large project going on at the moment."
            },
            {
                "timestamp": "06:54",
                "text": "For scientific hypothesis generation, it's called HypoGen, and one of our junior researchers is going to"
            },
            {
                "timestamp": "07:00",
                "text": "actually give a little talk himself in my talk."
            },
            {
                "timestamp": "07:30",
                "text": "And then we also have the community track."
            },
            {
                "timestamp": "07:59",
                "text": "So the goal here is okay we're building all these models AstroLlama I forgot to mention AstroLlama We also have a multimodal large model that we're developing How can we use these models to help the community around us?"
            },
            {
                "timestamp": "08:29",
                "text": "That's why we created a community track as well And one of the projects we're considering at the moment is adapting these large language models to help researchers that have dyslexia or other silent silent um challenges that affect their um their research."
            },
            {
                "timestamp": "08:59",
                "text": "Okay So where do we start when we think about NLP and when we think I guess more generally about artificial intelligence and I would like to start with the work of Dr. Alan Turing So in 1950 Alan Turing published this paper in a journal in psychology actually So you can see how forward thinking he was at the time and how you know how far ahead he was thinking."
            },
            {
                "timestamp": "09:29",
                "text": "And this paper computing machinery and intelligence in the very first sentence poses this very important question Can machines think?"
            },
            {
                "timestamp": "09:59",
                "text": "And to answer this question he proposes what we don't know as the Turing test uh where by you have a machine and a human and the human evaluator and the human evaluator only sees the machine and the human interacting via text the evaluator knows one of them is a machine but doesn't have much more information than that And if he said that if the human cannot distinguish between the machine and the human then the machine will have passed the Turing test It is very hard to explain this well you're a bit jet lagged but I hope that kind of gives you the gist."
            },
            {
                "timestamp": "10:29",
                "text": "Alright So what happened you know after 1950 we have 1966 this is Joseph Weizenbaum and my team He actually made this chatbot called Eliza"
            },
            {
                "timestamp": "10:59",
                "text": "So I want to demo you a Java and Java implementation of the chatbot that he he came up with And in this chatbot you talk to Eliza and say Eliza says I'll be your therapist today and you say well I'm feeling quite nervous because I'm giving a talk in front of an audience that's expert And then Eliza comes to me and says uh did you come to me because you're feeling nervous um Yes pretty much How can you help I mean how can you make me feel better um And then it goes fast about aggressive wasn't Don't you believe I can help It's contradictory I can but how have you asked such questions before Yes I'm pretty darn sure yeah"
            },
            {
                "timestamp": "11:29",
                "text": "Are you sure about that I'm pretty darn sure yes How long have you been pretty darn sure yes"
            },
            {
                "timestamp": "11:59",
                "text": "Right so this is probably matter It's Now fast forward to 2023 I go to ChatGPT and say maybe maybe something has happened in the last 50 years that can help I go to ChatGPT and say I'm feeling quite nervous today Now ChatGPT I think running with ChatGPT-4 has already validated my feelings It is already you know it wants to help um I told ChatGPT what why I'm nervous I say I want to ex I want to see how I can use large language models for science and I know I typed very slowly um It tells me I'm feeling overwhelmed and then it gives me solutions So after the interaction with ChatGPT I feel better that I say."
            },
            {
                "timestamp": "12:29",
                "text": "Okay so this has happened so we have 1950 Turing poses the question 1966 1966 we have Eliza and then we have now ChatGPT-4 running that one's for."
            },
            {
                "timestamp": "12:59",
                "text": "Okay So what has really changed between these two moments that we that we just have observed."
            },
            {
                "timestamp": "13:29",
                "text": "Many things have changed as we keep seeing if I was a PhD researcher right now in NLP I will be much more stressed than when I was doing my PhD in galactic archaeology a lot of advancements are happening a lot very very fast So here uh I put uh just some some uh big moments in the last 10 years So we have 2014 the paper about neural network translation that was a very very seminal paper for the field Then we have 2017 Vaswani et al coming up with a transformer architecture Then we have 2018 BERT and then you know we have this plethora of both closed systems and open systems On the closed system we have the work of OpenAI Google and so on but we also have the release from Meta we have Llama coming up and so on uh and now we have also work from EleutherAI and Alan Institute and so on."
            },
            {
                "timestamp": "13:59",
                "text": "So a lot a lot of models are out there."
            },
            {
                "timestamp": "14:00",
                "text": "images so you can you can probably talk more about this paper Um we have time series being models with transformers in astronomy and even distinguishing planetary transits um with transformer architectures So we see these models come back and back to life in astronomy and we are adapting to them sometimes in a better way sometimes not such a good way"
            },
            {
                "timestamp": "14:00",
                "text": "Okay so what what is the core of why these models work um especially you know in models like generative transformers why why do they work so the the response I normally give this talk in front of an astronomy audience but I know this is an expert audience so I won't go too much detail but we care about attention so if I ask you this question"
            },
            {
                "timestamp": "14:30",
                "text": "but most of the time we try our best But in at NASA ADS uh this paper arrived I think 2 years ago maybe I'm wrong 2 3 years ago called AstroBERT So this was the first paper that said okay let's think about other modalities where we express astrophysical knowledge and text came up as a really good way to do it So then AstroBERT came to life Um this model that was built by NASA ADS was used for named entity recognition in the NASA ADS corpus which at the moment has 5 million uh scientific articles is the largest one to date and it's also our partner for University of"
            },
            {
                "timestamp": "15:00",
                "text": "Penn Okay cool So all right So we need we need the maths right we need the attention we need we need the maths we need the people who work on these things at the frontier of knowledge But what else do we need to make this really work We need money We always need money Some of us need more money than others but that's important because if you're OpenAI you can actually uh try to you know exactly what happened here if you look at this plot you have the year and on the on the Y axis you have the size and you can see how um you know how OpenAI move from 1.5 billion parameters for GPT2 to already 170 billion parameters for GPT3 right"
            },
            {
                "timestamp": "15:30",
                "text": "And training these models means that you need to have access to resources GPU resources which cost money Okay So second thing you know you need is money And the third thing you need is human beings I would argue that that's the first thing you need but yeah so third thing is you need human beings because if you let the model um like GPT3 or GPT4 to do you know to to transfer in the way where is not aligned what you might get out we might not we might not like So people have spent quite a bit of time trying to align these models to um serve human interests right So uh or at least not be toxic in many many ways Uh some of us might disagree how how how successful this is"
            },
            {
                "timestamp": "16:00",
                "text": "but when you use reinforcement learning from human feedback you can try to achieve this task So you need maths you need money and now you need humans as well And now we are in 2021 with InstructGPT You all probably know this already So I hope I won't be going too fast or too slow Okay So how did it affect astronomy in this world What's up with astronomy Where where does astronomy lie This tweet came from Google and uh Google released this bot you know in a way to compete with ChatGPT"
            },
            {
                "timestamp": "16:30",
                "text": "And uh you know they're very hyped about it They're very very excited about it And then there was a human on Twitter X Twitter uh that asked what new discoveries from JWST can I tell my 9-year-old about And extra points for who gets this um in the audience that's really good But then by responded with JWST to the very first pictures of a planet outside of our solar system Now GPT I is right here you know people peeps they might not this might not like this answer you know so Okay"
            },
            {
                "timestamp": "17:00",
                "text": "it is wrong So what happened with the stock of Google when this happened He went down he went really down actually They lost a lot of billions that day That is if you're Google you could blame this on astronomy That's not good because people are like you can't lie to me about planets That's not that's one that I won't take Okay So we don't we at University are like let's do something about that So we were like okay"
            },
            {
                "timestamp": "17:30",
                "text": "let's move from a generalist model like Bard ChatGPT to a specialized model that we can develop as part of our community Okay So that brings us to AstroLaMA1 So this is the first iteration of the AstroLaMA model And please meet AstroLaMA1 uh we spent quite a lot of time in this journey trying to get that bring it to life And here I would like to credit the amazing work of George Nguyen who is now a PhD researcher at University of"
            },
            {
                "timestamp": "18:00",
                "text": "Pennsylvania If you want to learn more about AstroLaMA because Penn is quite close to here I would actually really have him here to come give the talk himself It's a really brilliant paper and he has done a great job And uh you know I'll talk a bit about the details of that more than in a minute But then we talked about it on Twitter because that's where all the information goes right uh good or bad uh And then we got a tweet from Yann LeCun and then uh this has become Letweet So the AstroLaMA and Letweet are very very proud uh parts of our portfolio"
            },
            {
                "timestamp": "18:30",
                "text": "Okay But how do you actually build an AstroLaMA So you have a base model you know you have the resources you have money you have a lot of data and then you have you you have a lot of patients You can wait for a long time to finish this training And then you get the pre-trained language model right So you know you're trying to predict the next token in an autoregressive manner"
            },
            {
                "timestamp": "19:00",
                "text": "With AstroLaMA what you have is you take the pre-trained language model and you adapt it to a data set that you have And I here I say small data set but it's actually should be smaller data set So once you do that you can do training uh with the causal language modeling objective on the small data set And then you get the fine-tuned language model"
            },
            {
                "timestamp": "19:30",
                "text": "So for LaMA for AstroLaMA we use the LaMA2 model we have used the 7 billion uh version at the time we are now training 70 billion uh we have trained I think 70 billion has been trained And now we have uh on this on this smaller model we have used 300,000 archive astro-ph papers So these are papers that we generally care about in astronomy which resulted us in a minuscule uh data set of 230 230 million tokens"
            },
            {
                "timestamp": "20:00",
                "text": "So um moving forward we have access to a lot more data coming up So we we can train these models uh much more Okay So with AstroLaMA for us AstroLaMA was the first step towards something uh bigger And we didn't we we just wanted to see what it does can we play with it"
            },
            {
                "timestamp": "20:30",
                "text": "So here I'm showing you an original abstract in astronomy And then uh you have three models So you have GPT4 LaMA2 and AstroLaMA And these three models just receive the same information we prompt them with the yellow part And then they complete the rest of the uh of the astr- uh abstract And when we look at the results what we see is that well first of all uh yeah AstroLaMA is not uh is not quite as perplexed about seeing astronomy language as the other two"
            },
            {
                "timestamp": "20:59",
                "text": "GPT"
            },
            {
                "timestamp": "21:00",
                "text": "And um in this you know I know that we are not in astronomy, but for me as an astronomer reading what it comes up with is much more the language that it uses is not quite as general GPT-4 is much more specific and even more importantly is it fails in more specific ways. So, it's much more detailed even in its mistakes."
            },
            {
                "timestamp": "21:30",
                "text": "Alright. But evaluation is an ongoing process. We are also learning how to evaluate things much better, but before evaluation also want to talk to you a bit about embeddings at the level of the astronomer so because we currently this model we are able to actually get the uh embeddings as well."
            },
            {
                "timestamp": "21:59",
                "text": "And here you get on the uh on the x axis the pairwise cosine similarity and then on the y uh on the y axis you get some density in some bins. So, here what we what we did is that we try to uh take um any two abstracts and see how similar they are."
            },
            {
                "timestamp": "22:30",
                "text": "And you can see GPT-3 that the shown in the blue in the blue bins that you know you are looking between 0.7 and 0.9. So, the way I think about it is that GPT-4 or GPT-3 in this case is sort of an island in the corpus of all the information that he has seen before."
            },
            {
                "timestamp": "22:59",
                "text": "Whereas AstroLLaMA because he has been fine-tuned for astronomy he is more than an island he is like an a a a continent of astronomy which is shown by this plot. But let me make this more uh more specific."
            },
            {
                "timestamp": "23:29",
                "text": "So, here you have two papers same with Bard right? So, paper one um talks about uh gyro kinetic kinetics and paper two talks about modified Coulomb law."
            },
            {
                "timestamp": "23:58",
                "text": "So uh GPT-3 similarity is 79% like these two guys are similar you know I'm I'm pretty pretty good with that whereas AstroLLaMA says not really they're talking about different things right? So, we have 36%."
            },
            {
                "timestamp": "24:29",
                "text": "So, we now because we are in the continent we can see how much further things are from each other. These two things are quite similar if you can see the I don't know New York Times um articles about uh I don't know something something very important."
            },
            {
                "timestamp": "24:58",
                "text": "Okay. Now we have a different example here. We have paper one that talks about uh a nebula and then you have paper two that talks about uh star formation in a region in a H II region. So, GPT-3 similarity for me as an astronomer these are quite similar things you know they should be talking about the same thing. They should be talking about what happens in the region of a nebula whereas GPT-3 similarity says 82% so yeah they should be similar but AstroLLaMA says this they should be much more similar because they are talking about the same yeah."
            },
            {
                "timestamp": "25:28",
                "text": "Yeah good question. Are these uh held out documents or these documents that work kind of like um so these are held out documents yeah. Definitely we we wanted to be uh as much as fair as we could in the sense but we are trying now to take a a smaller model like olmo where we know exactly what they're doing even before the pre-train the fine-tuning step."
            },
            {
                "timestamp": "25:58",
                "text": "Okay. Thank you. Alright. So, going back to Google now. University of the come to life for what happens to Google's stock uh that is exactly what happens to Google's stock.  No we we we wish they would give us some of those findings ourselves, but uh we move on."
            },
            {
                "timestamp": "26:28",
                "text": "Okay. So, moving into the next part of my talk um there is this question about um okay cool. So, we have a an a LaLaMa specialized LaLaMa that's really nice fine-tuning it works. And hugging face transformers is an amazing uh date, but can we go a bit deeper than that? Can we actually use these models to reason to generate scientific hypotheses which for a scientist and astronomer is the bread and butter of our jobs."
            },
            {
                "timestamp": "26:58",
                "text": "Literally. So, I would like to introduce HypoGen uh at this point which is the hypothesis generation. And I just would like to show you these very two quick slides um on hypotheses."
            },
            {
                "timestamp": "27:28",
                "text": "So, here I have told GPT-4 to only see 100 and 1000 papers. And then by through a broad pipeline so I'm talking to the model through a black pipeline. And then here on the y axis you have the evaluation score."
            },
            {
                "timestamp": "27:58",
                "text": "So, I said come up with an idea please and then uh and then we evaluated on the idea that it came up with using human experts. And we saw that we saw that the quality of these hypotheses well uh is not improving depending on the number of papers that he sees."
            },
            {
                "timestamp": "28:00",
                "text": "Uh with this very very small experiment is um truly just an a very small experiment And the work that's being done now by Charlie O'Neill who is working with us at the Australian National University I think next year he's going to apply for PhD He is one of the most brilliant people I've ever met Uh and his project HypoGen is being supported by Tirthankar Ghosal from  Aldrich National Lab and Dr Roberta Rilan from Meta"
            },
            {
                "timestamp": "28:28",
                "text": "So, this is very early work. This is uh in not in not any way statistically uh robust work, but we just wanted to experiment with GPT-4 at the time. And then we we did a trick. And then we we did a trick and we ask another model to criticize the first model and then provide the feedback to the first model."
            },
            {
                "timestamp": "28:30",
                "text": "And I will let Charlie actually tell you himself what the kind of work he's working on And I think we have some time Hey guys Um today I'm going to be doing a bit of a speed run of some research we did last year Um looking at how to um make language models more creative Um and yet more reliable and accurate at the same time Um and we do so with this thing called STEER"
            },
            {
                "timestamp": "28:59",
                "text": "Um so the problem is basically that LLMs as we all know are capable of generating very um creative synthetic data but often that comes at a trade-off of producing synthetic data that is coherent Um and we noticed this in many different applications but a one where it was particularly clear was generating scientific hypotheses Um methods to try and get it to be more creative and diverse such as increasing the temperature would obviously reduce the coherence of language model and we thought that there must be some way Um to address this that doesn't involve re-training the language model from scratch And so we thought why don't we just take the logits at the end of the language model"
            },
            {
                "timestamp": "29:30",
                "text": "And see if we can manipulate how we're decoding these somehow Um to produce data that's more coherent and Um more diverse So yeah this is an example of a scientific hypothesis Um so for instance if we drop the diversity right down so if we push it back to the origin here um to get more authenticity then we might get really authentic scientific hypotheses um in astronomy but these are pretty um standard hypotheses that have been said many times and it can It sort of just says the same thing over and over again Um if we increase the diversity by increasing the temperature and repetition penalties and all that sort of stuff"
            },
            {
                "timestamp": "30:00",
                "text": "We get very interesting hypotheses like uh black holes made of cheese but um they're not very coherent Um they're not very good scientific hypotheses So we noticed this sort of Pareto boundary of the trade-off between authenticity and diversity And we want to see if we can push that boundary out and get something that's both more authentic and more diverse"
            },
            {
                "timestamp": "30:30",
                "text": "So in terms of background there have been a lot of attempts to manipulate the logits of a language model in order to um increase some dimension of the output Um so for instance classifier-free guidance which originally came from diffusion models Um upweights the importance of the prompt so it looks at the logits with and without the original prompt and can subtract um the logits without the prompt um from the logits with the prompt And that is a way of emphasizing the importance of what you're feeding in And this is actually used in system prompts for many language models now Um contrastive decoding coherence boosting context-aware decoding Um they all involve some subtraction of the logits distribution with something and the logits distribution without something Um and so yeah I guess the common theme here is steering the language models decoding uh by subtraction And remember this is just the last layer this is the logits that come out of our final projection This isn't re-training at all Um which is handy cuz it means it's available to people with"
            },
            {
                "timestamp": "31:00",
                "text": "Few with not much compute So our methodology and the key idea here is we want to increase the quality of two dimensions at once So all the previous things we just mentioned increased um one dimension at once for instance adherence to the prompt So to do two dimensions at once we sort of reason that we probably need two different types of subtractions So if you imagine um your real data distribution let's say of of astronomy hypotheses Um being in some support and some high dimensional space um to achieve authenticity we want to attract synthetic examples or generated examples with the LLM to that Um real distribution And so our attractor here is what we call contrastive expert guidance So all we do is we take a fine-tuned model so maybe a model fine-tuned on astronomy papers"
            },
            {
                "timestamp": "31:59",
                "text": "Um and we get the logits from that And then we subtract the logits of an unfine-tuned model so the same model Um but unfine-tuned And so what that does is it attracts the generated examples to that Um real data distribution But at the same time like we said before if we attract too much then we end up with examples that aren't diverse And so we need a repeller at the same time And that's negative prompting So we randomly sample from things we've already generated and from existing real examples from that real data distribution And then we subtract the logits of the prompt with the negative context So in that case we're telling the model to avoid things we've already generated And if we can balance these two together Um as shown by these equations here Um so this is parameterized by two different hyperparameters these subtractions If we can balance that correctly then we can actually Uh well we hypothesize that we could achieve an increase in authenticity and diversity at the same time And so this is a little picture here so this is the attractor so we're making our examples more relevant to the real data distribution rather than the general distribution of language And negative prompting reduces the likelihood of things we've already generated"
            },
            {
                "timestamp": "32:59",
                "text": "So some results So we tested this on scientific hypotheses astronomy hypotheses Um uh Jigsaw which is a um toxicity data set of toxic and non-toxic online comments And a common sense question answering data set And so we fine-tuned a model on this and then used our STEER method Um so the two subtractions when we were decoding Um to generate synthetic data And we did so with some other benchmark decoding methods such as top-K nucleus and contrastive decoding which just sort of state of the art decoding methods And we found that according to our Um automated metrics here that we achieved high scores in both diversity and authenticity with our STEER method which was a good start"
            },
            {
                "timestamp": "33:30",
                "text": "We also did some human evaluation and GPT-4 evaluation And again found that for the right combination of hyperparameters that governed the attractor and the repeller um we achieved significantly better Um win rate against uh nuclear sampling with our STEER method A bit of a sensitivity analysis which is just confirming that as we increase the attractor Um hyperparameter um that we get more authenticity at the cost of diversity and same with the Um repeller hyperparameter data versus So again ablations showing that there is a definite improvement Um when we change these um attractor repeller hyperparameters from zero"
            },
            {
                "timestamp": "34:00",
                "text": "Another way to measure the quality of our data we've generated is um with downstream accuracy so if we can train a classifier on the Um synthetic data um to do the original task so if"
            },
            {
                "timestamp": "35:00",
                "text": "For instance Jigsaw toxic comments CommonsenseQA we can generate synthetic data and train a model on a downstream task eg text classification This is a test of knowledge distillation."
            },
            {
                "timestamp": "35:30",
                "text": "For the ArXiv Hypotheses we can examine the win-rate of different generation methods against the real data using expert evaluators."
            },
            {
                "timestamp": "35:45",
                "text": "And so we see here that STEER the performance of the classifier and the question answerer was significantly better than Greedy Nucleus and Contrastive although still not quite as good as real data which is showing that there's probably more to do."
            },
            {
                "timestamp": "36:00",
                "text": "And there this is sort of again showing going back to that Pareto boundary showing that we've actually pushed this out so for different combinations of the hyperparameters for theta and gamma the attractor and repeller we get a performance increase in both fidelity and diversity over Nucleus and Greedy."
            },
            {
                "timestamp": "36:30",
                "text": "So why do we care about this I guess now that we've got these really large language models there's a lot of talk of GPU poor if we can't afford to fine-tune these huge models for a particularly long period of time we have to use smaller models and find ways to get more improvements out of them."
            },
            {
                "timestamp": "36:50",
                "text": "And so we think the methods like STEER and this sort of decoding methods at inference time can be really important for generating synthetic datasets that are diverse and authentic at the same time."
            },
            {
                "timestamp": "37:00",
                "text": "A follow-on from this is perhaps generating synthetic datasets that we can use to retrain these smaller models for recursively improving language models and there's been some work on that recently as well."
            },
            {
                "timestamp": "37:30",
                "text": "And yeah in terms of additional work we also see that other people have since done similar things to us."
            },
            {
                "timestamp": "37:45",
                "text": "People are actually doing this subtraction not at the logit stage but even in the weights of the model in order to come up with task vectors so it doesn't just have to be authenticity and creativity which are dimensions you can use any arbitrary dimension in the model and as long as you have a way to define that task vector you can actually perform addition with those vectors and steer the model towards whatever dimension you care about."
            },
            {
                "timestamp": "38:00",
                "text": "And so yeah wrapping up hopefully the logic behind this method this brief introduction gives you food for thought about how we can generalize this attracting and repelling idea with smaller language models."
            },
            {
                "timestamp": "38:30",
                "text": "And maybe ask you to come up with some other ideas of subtraction which might improve performance so everything from subtracting unconditional distributions to short versus long contexts etc."
            },
            {
                "timestamp": "38:45",
                "text": "And yeah thanks to UTBD my supervisor Tang and obviously Yuan-Sen and Joe for all their assistance on this project."
            },
            {
                "timestamp": "39:00",
                "text": "Alright so we'll continue."
            },
            {
                "timestamp": "39:15",
                "text": "So Charlie will probably do a US tour next year and I would really yes."
            },
            {
                "timestamp": "39:30",
                "text": "I just want to clarify questions because the speakers but you can get the speakers if I have did I understand it correctly that you measure the diversity here in the you have to Yeah."
            },
            {
                "timestamp": "39:45",
                "text": "The metrics that I understand they are measuring diversity in terms of shadow measures such as the diversity and the diversity of different phases and things Yeah But I think this is the kind of diversity that you have in mind is like how creative and innovative this is and these are very different forms of creativity Yeah Your concern is very valid and we are actually So this work has been done a year ago and since then Charlie has been working on the current hype-driven model which we are actually hoping to get a paper released from UTBD data sets workshop where we create a scientific dataset and we also hopefully will provide some metrics that we curated to be able to evaluate on a dataset."
            },
            {
                "timestamp": "40:45",
                "text": "And with those metrics we are taking this into account and human feedback is part of it And also we have a set of GPUs that also play a role into doing some sort of model evaluation themselves So there is a lot of talk about thinking Roberta is helping us quite a lot on the metrics side Yeah Okay."
            },
            {
                "timestamp": "41:00",
                "text": "And at later on this time I would love to share those metrics with you Absolutely and I would love to put you in touch with Charlie He's really spearheading this effort and he's a brilliant actor person so I think he would also appreciate talking to you."
            },
            {
                "timestamp": "41:30",
                "text": "Thank you very much Um okay so let's keep going because we have 20 minutes to finish quite a lot okay So AstroLaVa we have AstroLama now we have hypothesis generation But we also thought about doing something fun to give back to the community that wants to engage with our models So for this reason we thought that a picture is worth a thousand words so not going to go to the beautiful images of galaxies."
            },
            {
                "timestamp": "41:50",
                "text": "So here we talk about the LaVa model the LaVa model is a clip encoder you know that's hooked to a to a decoder and then what happens here is that you have this vision encoder having been frozen but you have this connector part here you have this vision language connector which is very fancy metrics and then with this metric you can actually sort of change the weights in such a way that you can connect these two modalities in a latent space."
            },
            {
                "timestamp": "42:00",
                "text": "next level with scientific applications, for example, images from JWST."
            },
            {
                "timestamp": "42:00",
                "text": "And then the good idea about LaVa is that once the training is done in the way that you are changing the weights of the MLP there and also the language model you can actually pass new images through the model and get the text done because suddenly now the model understands how to align text to image."
            },
            {
                "timestamp": "42:05",
                "text": "Cool."
            },
            {
                "timestamp": "42:07",
                "text": "So, my favorite part of our UTBD, apart from working with the most incredible people, is this part, the community-focused part. So, how will UTBD support and inspire the community? And here, uh, we have a few projects, but I just want to mention two very briefly."
            },
            {
                "timestamp": "42:22",
                "text": "The first one is called Pathfinder. This project is being spearheaded by Dr. Karthik Iyer. He's currently a Hubble fellow at Columbia University."
            },
            {
                "timestamp": "42:29",
                "text": "And that's him right there. Um so, Pathfinder is really looking at the arXiv for astronomy, and we're trying to work at the embedding space level."
            },
            {
                "timestamp": "42:30",
                "text": "So training the AstroLaVa again a work that's headed by two of our brilliant pieces in UTBD Sharof and Mike Smith And what they've done here they worked they went to the astronomy picture of the day and they looked at API pictures and looked at descriptions of the picture and then we created a conversation dataset based on that description and then we created this training set and then we fine-tuned LaVa to create AstroLaVa."
            },
            {
                "timestamp": "42:39",
                "text": "So, that we do very good question answering, and currently I think we're running, uh, we're we're trying to use the AstroLlama as the base model behind the, the log."
            },
            {
                "timestamp": "42:49",
                "text": "So, what happens here, you can ask a question, and then Pathfinder brings you the most relevant papers to answer that question. It also gives you meaningful citations, and hopefully this aids, uh, you know, our community at a very, uh, sort of absolutely necessary level because astronomy and astronomers are still, um getting up to date with these advancements in, uh, in machine learning."
            },
            {
                "timestamp": "42:50",
                "text": "And because AstroLaVa is still making some mistakes that we are quite scared about we are still evaluating the model to be released very soon."
            },
            {
                "timestamp": "43:00",
                "text": "But the way the community will interact with it will be by Hugging Face and you can put in a very nice image of your favorite astronomical object and then you can learn more about that particular object So in this way we hopefully inspire the community to think about the great beyond as well as think about the great research ideas that you can do with a model like this We also have now people joining UTBD who are going to take this to the next level."
            },
            {
                "timestamp": "43:12",
                "text": "So, yes, we have this already available online, and I, you know, if you ever want to learn astronomy, and this is the place to go, uh, for Pathfinder."
            },
            {
                "timestamp": "43:23",
                "text": "So, you know, another project that we can do with Pathfinder is trying to do these visual cues visualizations here where you can find islands in astronomy. So, you just take the data from papers, you have the embeddings for that dataset, you project them into some dimensional or lower dimensional representation, and then you start, start looking for, um, patterns. So, you can see here that you have the JWST Valley."
            },
            {
                "timestamp": "43:47",
                "text": "Now, we haven't yet done a temporal analysis, but I'm pretty sure that the JWST Valley has seen, uh, emergence in the last 10 years."
            },
            {
                "timestamp": "43:57",
                "text": "All right. And my favorite part, uh, about, uh, um, about UTBD is that we can also think about, you know, what's the space between, between the, uh, what's the space that we do not see actually? And here, I would like to talk a bit about AstroLyla. "
            },
            {
                "timestamp": "44:14",
                "text": "So, AstroLyla is truly a variation of the AstroLlama that's aimed at improving accessibility for researchers everywhere."
            },
            {
                "timestamp": "44:21",
                "text": "And what do I mean by that is that, you know, we now live in a world where scientific output has been growing, uh, by 9% per year over the last decades, so this is, you know, booming in scientific literature."
            },
            {
                "timestamp": "44:32",
                "text": "Only science and engineering, so the field of astronomy, um has seen 2.5, 2.9 million articles published in 2020, and this growth is increasing."
            },
            {
                "timestamp": "44:44",
                "text": "That's a lot, that is a lot of papers, right? And it is a struggle to keep up, um even if I just read the abstracts, it is truly a struggle to keep up."
            },
            {
                "timestamp": "44:55",
                "text": "So, now I would like, I would ask you to try to do this exercise. So, think here, I have written an astronomy abstract, and the words have been shuffled, and the letters have been shuffled inside, uh, the letters have been shuffled inside the words."
            },
            {
                "timestamp": "45:08",
                "text": "So, the first thing that you try to do, your brain tries to do when you see this is try to rearrange the letters, and then try to put the words back into where they should be, and then it does the comprehension steps to actually understand what is happening."
            },
            {
                "timestamp": "45:21",
                "text": "So, once we do it, you know, if we do it, uh, we do it quite fast, you know, we put the letters where they are, uh, now the words are also where we expect them to be."
            },
            {
                "timestamp": "45:29",
                "text": "And finally, we are like, yes, we're starting paying attention to, uh, to, to where the comprehension step. So, in this case, this abstract is talking about what happens if you have our star, the Sun being close, being, uh, younger than the galactic bar of our galaxy."
            },
            {
                "timestamp": "45:45",
                "text": "What happens if this is, uh, uh, a scientific question for us in galactic archeology."
            },
            {
                "timestamp": "45:51",
                "text": "So, if the bar is younger, you know, the, the Sun would move quite a bit, for example, because the bar has some dynamical effect on the Sun, on the Sun's orbit."
            },
            {
                "timestamp": "46:01",
                "text": "Okay. So, why dyslexia? So, I think this small exercise has already shown you why we care about dyslexia. Uh, my best friend, uh, and many of the people that I, I, I, I am surrounded by in astronomy, many of them tell me that sometimes they think they might struggle with dyslexia. So, it's, it's a difficulty in decoding, uh, written language."
            },
            {
                "timestamp": "46:23",
                "text": "And, it's 10% in the general population, but it's believed to be much higher in the academic group."
            },
            {
                "timestamp": "46:30",
                "text": "And, people who have dyslexia experience things like cognitive load, and slow reading speed. What is cognitive load means, you know, you, you get tired after you try to read a paper that has 50 pages in your field."
            },
            {
                "timestamp": "46:45",
                "text": "So, what might take me or, uh, one of my peers who doesn't have dyslexia, 1 hour reading time, might take a colleague of mine who does have it 2 hours reading time. So, now the question emerges, what happens if, down 1 hour, how will that 1 hour affect people's career long term?"
            },
            {
                "timestamp": "47:05",
                "text": "That is a valid question that I think we can try to solve at UTBD, or working in the direct, direction of providing some solutions."
            },
            {
                "timestamp": "47:14",
                "text": "So, why do we care, what can we do about it? We want to mitigate this problem, uh, for astronomers, and long term we want to mitigate this problem for people everywhere. Um cool. So, the way we go about it, we can adapt AstroLlama, right? That's that's the beauty of these large language models, you don't have to work on the, uh, most incredible like, uh benchmark performance, but you can actually take these models and try to adapt them to something that matters for people. So, we can fine-tune AstroLlama on some identified tasks that might help."
            },
            {
                "timestamp": "47:45",
                "text": "Summarization, sentence simplification, and so on. That is, you know, aligned with the scientific need, and then we can evaluate it, and see how well it's doing by working with colleagues of ours who have dyslexia, and taking their feedback and putting this back into our pipeline. And then, uh, after we'll engage a larger community so we can do this, uh, in a much more, uh, efficient and fast way."
            },
            {
                "timestamp": "48:11",
                "text": "Okay. So, I think that's me. I think we're done. Thank you so much. So, before, uh, before we finish, I would like just to say that we have our Slack channel, and that's where we sort of do the bureaucracy, bureaucracy very boring part of stuff. But we also have a Discord channel, which is truly community facing, and we are welcoming people everywhere to try to join us and help us in our mission. So, please if you'd like, um, to join, please join us on our Discord. And if you want to chat with the AstroLlama, we are now, we have released also, uh, the AstroLlama Chat model, uh, very recently. So, it's Hugging Face, the AstroLlama Chat. Thank you so much and thank you for having me here."
            },
            {
                "timestamp": "48:55",
                "text": "Now, we have time for two questions. Uh"
            },
            {
                "timestamp": "49:00",
                "text": "Thank you."
            }
        ]
    },
    {
        "video_id": "LIDzV7zXTi4",
        "transcript": [
            {
                "timestamp": "00:00",
                "text": "June 18, 2024 AstroAI 2024 Workshop AI SpAI with My (Little?) AI: Applications of Statistical Learning in Modern Astrophysics Hello world! Joshua S. Speagle (\u4f50 \u52a9\u624b) Department of Statistical Sciences David A. Dunlap Department of Astronomy & Astrophysics Dunlap Institute for Astronomy & Astrophysics Data Sciences Institute UNIVERSITY OF TORONTO Um we're going to first have an announcement from Jill here and she's going to then we're going to get into our speakers real quick It's about free money so very important Um so I'm part of a project where we're trying to digitize historical astrophysics literature so if you've ever tried to read a paper from like the 1970s and you're like wait why can't I highlight anything why can't I extract figures We're kind of trying to solve that Um but it turns out that that's a really tricky problem for machine learning and AI to do still Um so we have designed a uh a a universe sort of universe Um a citizen science platform on the universe uh to try and do that sort of which with a large-scale citizen science community And we are trying to um understand how people use this interface from uh folks who will be considered experts like all of you um to kind of the everyday lay person citizen scientists So um we are recruiting It is a online um It fires like this on the little table that's sort of where the ledge that's right behind that uh that wall over there It's got a QR code It's got a URL uh and so if you're interested it is half an hour it'll be online We'll send you an email about it in a couple of weeks uh and you get a $1 gift card to Amazon So again it's free money so it's pretty it's pretty good hourly rate I'd say Um and I will say I'm Jill now and so if you want to talk more about what goes wrong with machine learning on that and uh I kind of different flavor of things Also here's a cap about that And thank you for letting me take your time up  Alright"
            },
            {
                "timestamp": "00:00",
                "text": "So, in low dimensions, there's not really much of a benefit, and in fact splines are great and have lots of cool properties. Um, and higher dimensions is really where, where these methods shine. Um, but fundamentally, it's an excellent question. They, they behave very similarly. "
            },
            {
                "timestamp": "00:30",
                "text": "Thank you very much Jill So uh it is my absolute honor to announce or to introduce the first speaker on the first AstroAI AI workshop Um There's a lot of pressure on you It's Josh Speagle So I met Josh as a uh student here when he was a grad student here at Harvard and uh he did an undergrad here as well Yeah And uh and uh we in the uh in our research group quickly realized this guy has got some chops He knows what he's doing when it comes to stats He knows what he's doing with astronomy and that really has translated into a very diverse research package that you you've brought to astronomy And um so he finished up his grad work here at Harvard and went on to the University of Toronto as a post-doc fellow fellow and then continue on as a joint at uh appointment at the University of Toronto uh faculty in the uh staff department and also the astronomy department Um so again he has a great research group up there They do tons of stuff a lot a lot having to do with the the intersection between big data uh stat statistics astronomy and and also AI now So um anyways uh I want to just uh say thank you very much for opening up our workshop and uh and I'll pass it over to you Alright"
            },
            {
                "timestamp": "00:30",
                "text": "Okay. So, uh, I think the, the good thing is that we definitely won't get through half the stuff on the top, which is great because I think people would be very confused if I tried to rush through it. Um, so, I'm going to keep going until we sort of get to, to a nice stopping point and then I'll jump up to the end, and people can ask me more questions if they want. "
            },
            {
                "timestamp": "00:59",
                "text": "Alright let's see if I can actually turn this on properly Alright Okay uh is that coming through I can hear me Okay Excellent Well thank you for the wonderful introduction Phil It's always great to be back Um yeah so as Phil mentioned I did my uh bachelors and masters in PhD here Um but eventually I had to leave And so I ended up in Toronto As he mentioned I have this this fun joint appointment between statistics and astronomy Um and also affiliations with the Dunlap Institute and the Data Sciences Institute By the way in case you're wondering these are actually two separate things Um same family but just like 100 years apart Um so if you want to know the backstory behind that please come talk to me Um And yeah I think to preempt all this uh so I've set expectations as low as possible This talk is sort of when I saw an hour and a half I'm like what should I do And then of course in classic fashion I sort of just tried everything So this talk is sort of 50% kind of pedagogical 50% kind of illustrative science and 100% unhinged So what that means is that if you are confused at any point you have any questions you have comments on stuff just raise your hand or if I'm not looking at you because I'm too busy like just flailing wildly please just shout it out Um you know I want to leave plenty of time for questions throughout all of this especially because this is the first time I'm giving I think uh talk in this particular format Okay"
            },
            {
                "timestamp": "00:59",
                "text": "Okay. So, what this means in practice for machine learning, and ultimately we can think about in terms of AI and astronomy, is that there's going to be a plot that we can make like this, right? So on the x-axis there's something like the number of data points that we have or the amount of compute that we're willing to dedicate to our model. Um, on the y-axis, is going to say how good our model's performing, right? Sort of worse is on the top and better is on the bottom. "
            },
            {
                "timestamp": "01:29",
                "text": "And, there's going to be some thing like this, right? There's some starting model that will kind of do this. This is what we expect. In other words, the more data we have, the better our model will do, the more compute we have, the better our model will do. Um, so the question, of course, the real thing that we care about is what is the slope of this relationship, right? It will always reach zero eventually, but how quickly? Um, and that could change depending on the exact model. Right? You might not expect that all models have the same. They might have some slightly different slopes. Some might converge faster, some might converge slower. Um, that's that's going to be something that we might care about. "
            },
            {
                "timestamp": "01:59",
                "text": "The second is the normalization, right? What if you have a model that is just much better with the same amount of data? Or, another way to think about it is for this model is equivalent to another model with 10 times or a 100 times more as much data. That's really a better way that people are thinking about machine learning now, is that no matter how terrible my model is, if I give it enough data, it will be okay. But, maybe I could give it some less data, or maybe I just don't have enough data so I can't just rely on scaling to infinity. Um, also, maybe we don't have like a billion dollars to like just burn on compute every day, you know? I mean, the numbers are very high. "
            },
            {
                "timestamp": "02:29",
                "text": "So, that's sort of what I would call part two. It's really thinking about scale, right? Like, in astronomy where are we? Um, I think you all sent later this week, we'll probably talk a bit about this, sort of an AI perspective. uh, for large language models, but, I just want to talk about this from a more fundamental point of view with these ideas that we've now covered in the, in the first part. "
            },
            {
                "timestamp": "02:59",
                "text": "Um, so, this turns out to be super general. So, this is a result from Caplan oil. Sort of these like neural scaling laws. And, so the left is showing eventually compute. So, again, you get more compute, the models all converge, they do better, there's sort of this limit. So, great, uh, dataset size, this is sort of uh, NLP stuff. So, you had more tokens, the model does better. Um, and then parameters, uh, same thing, right? All scales, and these are log log plots, essentially. So the best for astronomy, right? You get these power law scaling relations everywhere. So, this means if you want to do, you know, some fraction better you need to add some fractional amount of data. Right? So, you know, 10 times more data points or 100 times more data points gives you some fractional improvement relative to where you were. "
            },
            {
                "timestamp": "03:00",
                "text": "So, I want to start by saying this is work that's been led by Mike Walmsley who's a Dunlap Fellow at Toronto."
            },
            {
                "timestamp": "03:00",
                "text": "So before I get started I just want to say that uh you know the work that I'm presenting here um is only a small portion of sort of the work that we're doing at the University uh of Toronto So this is part of what we called the Astrostats research team or the ART uh That's our logo up top which we're very very proud of Um for someone who does a lot of work on supernova you might even recognize that the position of the stars might look uh very similar to the Safirson Hubble's diagram uh from 19 uh from when he first published uh Hubble's law Um So this is a a co-led research group between Gwen Eadie who's another joint professor between statistical sciences and uh astronomy and astrophysics as well as myself We currently have five postdocs with a few coming in uh nine PhD students and a whole uh set of undergrads This is our picture from last summer We're hoping to replace this later this summer If you're interested at all uh feel free to contact either me or Gwen um or visit our website at astrostat.uoft.com Um We're super excited and we work on a whole uh range of stuff So the unifying theme for this talk and also for the research group is really going to be statistical applications in astronomy Um and so that's a lot of what we'll be focused on"
            },
            {
                "timestamp": "03:30",
                "text": "Um, Mike is really the uh is involved with the Zoo Universe team, which does a lot of citizen science work um to try and label and sort of model uh different images for a bunch of different uh science cases."
            },
            {
                "timestamp": "03:59",
                "text": "Okay So you know this is supposed to be AstroAI am the first one also speaking so I thought I might as well give uh a very broad introduction Um And so you know we're all here right because of machine learning Now I should replace this with AI Um but the idea is right like we see it everywhere it's using all aspects of our lives Um and indeed it's uh it's now been around long enough that this meme about machine learning itself feels a bit outdated Um So so it's a natural question right why is it everywhere And so the immediate answer is that it's very fast right like you have a lot of data you a limited amount of time old sort of computational tricks weren't really cutting it for a lot of applications And so machine learning sort of took in as a sort of this very efficient way of processing data Um I think more more interestingly though a lot of people use it because you know as people say it just works Right You train a model on a particular data set and it often consistently outperforms traditional approaches I put this in asterisks of course because you know what exactly we mean by traditional changes But the fact that machine learning is really good Um is sort of proven every day when I go on the archive and I see a paper that says we applied machine learning and it does better than the traditional thing and that enters like exhibit 1 million 537 uh for machine learning winning Um The final thing is that it's very flexible right It's very easy to get started with packages like scikit-learn with things like TensorFlow PyTorch Jax Um So it's very easy to use And then it turns out that by starting with the same general frameworks you can actually apply the same methods on so many different"
            },
            {
                "timestamp": "04:00",
                "text": "Um, the original one is Galaxy Zoo um which is what Mike's involved with, and so Mike essentially had this question of if I have the largest ever human labeled astronomy data set with 830,000 galaxies from a 107 million volunteer clicks, um how well can we do because no other data set can kind of rival this amount of information, at least right now."
            },
            {
                "timestamp": "04:30",
                "text": "Um, so he took data from a bunch of different surveys. This is from SDSS, DECaLS, um Hubble, HSC, and UKIRT. Um, the publications and things are listed at the bottom. Um, and sort of asked like if I train models and try these things out, what do I see?"
            },
            {
                "timestamp": "05:00",
                "text": "And, below and behold, you get the scaling laws for galaxy images. And so, the questions of course are like what happens with galaxies, which do not look like the typical images that are fed into lots of models, right?"
            },
            {
                "timestamp": "05:30",
                "text": "Uh, it'd be great if uh, you know, in the sky I could see like a nice cat or something, but that's not really what uh we have. Um, and what does this sort of imply for for what we should be doing for citizen science, right? Every time we go and collect another image, we're getting a decreasing return. Um, once our data set gets large enough. So, we need to really optimize where you want to put that benefit in because uh essentially every click is sort of becoming uh less and less informative, or at least less and less useful in general for these models."
            },
            {
                "timestamp": "06:00",
                "text": "So, that's the idea. Train bigger models on more data. Test how they do. And then also test what they do on new tasks. I'm not going to talk about the last one um which Mike uh has done a lot of interesting work on too. Um, but just to sort of highlight this is kind of where Mike was uh was exploring. So, all of the different dots are from previous work, and what Mike has been doing by looking at sort of labelled galaxies and number of parameters is sort of up here in these two boxes."
            },
            {
                "timestamp": "06:30",
                "text": "Um, so I'm mostly going to talk about kind of this uh, you know, upstream and downstream type stuff, which is all, as you can see bigger than almost everything else that's been done."
            },
            {
                "timestamp": "07:00",
                "text": "So, uh you know, Mike burnt lots of a few hours. Um, but the end result was getting essentially this plot, right? This was really it to make a bunch of power loss. Um, so, this is what it looks like for a whole range of architectures trained to to look at images. So, Resnet is kind of like the old school like image one, and you can see other ones like EfficientNet uh different versions of EfficientNet, uh MaxViT, visual transformers. Um, then also ConvNext, uh different sizes of those models. So again, this is the same architecture just with more and more parameters in it."
            },
            {
                "timestamp": "07:00",
                "text": "different problems which is kind of incredible right And what other area would you say that oh I can you know take some bespoke methods that sort of does a bunch of weird stuff and then like it just works in diverse domains ranging from chemistry to physics to natural language uh you know prediction and everything else So that's kind of incredible right that it actually does that So what I want to sort of do in this talk is to really start thinking about or why exactly this happens and is it really so simple right"
            },
            {
                "timestamp": "07:30",
                "text": "And, below and behold, all of them display similar types of scaling laws. So, they all do better when you give them more data. So, that's great, right? So more training data improves performances, and we sort of look at the power laws for this. So, that's that's what we expect."
            },
            {
                "timestamp": "07:30",
                "text": "The easy answer is no right Whenever I remember a paper at some point in the archive that said how many uh papers with questions for their titles is the answer yes or no and the answer was almost all of them were no Um So but I'll caveat it by saying it's just going to be thinking carefully about what we really care about And I will even go further to illustrate exactly what I mean especially in the first part of this talking So a lot of times we're using machine learning it's really important to think about what we're trying to do with it Um Are we just trying to predict a function"
            },
            {
                "timestamp": "08:00",
                "text": "Um, and the training data has a greater effect than the architecture. Again, same thing, right? Um, So, you can always get eventually a model that starts off being bad to be good. It just takes longer. You just need more and more data points. One way to think about this is if you draw sort of a line about, you know, how many images it takes to get the same performance, or sort of saying here for example that's sort of a bigger Resnet model. Um, essentially can do as well as as a smaller one with, you know, 10 times less data."
            },
            {
                "timestamp": "08:00",
                "text": "Are there any astrophysical constraints that we really care about Do we need to understand why or how it works Is interpretability sort of a key concern Um What about errors quality flags You need to know when your model might fail or what happens when you feed in data that's maybe very noisy versus data that's not so noisy a common problem in astronomy And then of course what happens if things go uh go wrong right The nightmare scenario right is if we have some complicated machine learning algorithm that triggers a targeted opportunity and you slew JD VST over and it's a and it's a bug right That's like the worst nightmare you could have Ultimately what this really comes down to what I want to emphasize is that drives a lot of the stuff I'll talk about is how do you actually get people to trust your results"
            },
            {
                "timestamp": "08:30",
                "text": "Um, similarly, you can see that some of these other models, like this very efficient uh convolutional sort of network transformer, that these can essentially do as well as sort of these final models, uh you know, pretty quickly. So, they require, you know an order of magnitude plus data."
            },
            {
                "timestamp": "08:30",
                "text": "Why is it that you see all these papers that are published all the time doing these new sort of fancy machine learning methods on the archives But yet when you look at sort of how surveys look at data the types of analysis that are making their way into these key science projects in cosmology and stell astrophysics and galactic astrophysics and X-rays Um why is it that people still tend to use simpler approaches still tend to use more statistics driven approaches right The ultimate answer always comes down to trust How exactly do you get people to trust that your algorithm is doing what it's supposed to be doing"
            },
            {
                "timestamp": "09:00",
                "text": "So, this kind of gets us both of those questions from earlier. Yes, we can always use more data, but being clever still helps you especially when you're fixed on the data size."
            },
            {
                "timestamp": "09:00",
                "text": "So this is of course uh now this is really out of date Um This is back when Marvel was good So hopefully it's not too out of date Um But you know this is really exactly what what we mean and I'll actually clarify this more since uh since we'll come back to this concept So the motivating questions I want people to think about as I go through this cuz I'm going to cover a huge swathe of material all over the place is trying to think about what exactly machine learning can teach us about data and about fundamental astrophysics"
            },
            {
                "timestamp": "09:30",
                "text": "More parameters also helps, but up to a point, right? Like the the the amount of information contained in the data is finite. And so, if you have a model that becomes infinitely complicated, but you just don't have enough data to take advantage of it, your model doesn't do better. So, this is the same thing, but now showing two different sets of images, where the same types of models as before, and again, you see there's an improvement and that this does scale with the number of parameters, right? So, if you have more images having more parameters does help. You see the improvement essentially is bigger for these."
            },
            {
                "timestamp": "09:30",
                "text": "and how important it is that that sort of our model is tuned to that Um How can we use this to aid in sort of data driven discovery Um and I'll sort of show some examples of how I think about that and what this means And then also how can we move away from thinking of the intersection between say machine learning AI and astronomy versus machine learning for astronomy right trying to really think about how do we develop algorithms that are really designed and tuned to serve sort of the astrophysics community both in terms of actually being used in production on real science and real surveys but also that is really designed to work well with the data that we have which is not always the same as other fields of course"
            },
            {
                "timestamp": "09:59",
                "text": "Um, but you know ultimately the thing that matters is that you have enough data to actually justify the big model that you're fitting. So, this does actually imply that we have a data problem, right? We need larger training sets to take advantage of these very complicated models, and right now, at least, um for the data that we're looking at, uh the models saturate around 100 million parameters. So, it's not really worth going bigger than that just because the information, content, the data appears to be like capped."
            },
            {
                "timestamp": "10:00",
                "text": "So this will be explained to a couple of different parts which uh I'll sort of go through at some random speed Um So if again if you have questions please let me know But first place I want to start with is going to be confusion which is that uh some of you will probably see what's coming and some of you will probably be as shocked as I did when I first realize this is happening But the question is why exactly does machine learning even work Um And what I wanted to say for this is introduce a very old statistical concept So I'm in the stats department so I'm I I apologize in advance I'm going to talk about bias variance trade off I'm going to talk about linear regression you'll often talk later this week which includes even more great estimators Um about how exactly you can apply this to various things too Um So what exactly do we mean by bias variance trade off"
            },
            {
                "timestamp": "10:30",
                "text": "Conceptually speaking it's just that when you have a model Um you uh you essentially just have this sweet spot So your data essentially has some complexity to it You need to model that complexity with your model If you make your model a little bit too simple then it doesn't perform super well on both the training and the testing data And then as you sort of you uh add more parameters to your model like it gets better and better and better So this is what we call like the training risk or the performance It sort of decreases"
            },
            {
                "timestamp": "11:00",
                "text": "and you start to see the emergence of this gap often between the training and the testing data in other words data you haven't seen before And there's a minimum right where essentially this reaches its peak where you sort of balance between underfitting and adding model complexity and overfitting which is when you start to essentially predict or overpredict noise in your data And of course the worst case where that this happens is essentially what's called the interpolation limit which is you have for example a bunch of observations and rather than trying to say fit a line through them which is a pretty good fit you instead do something like this where you fit every single data point Um and you technically get perfect prediction but this model's probably not correct There are lots of different ideas about how we can think about this But this is one way to illustrate this process Um So you have what's called the capacity of the model which you can think of as the number of parameters say in a polynomial here and you also have the actual uh performance on on the data right If you make your model more have higher capacity that has more parameters you need to tune then the performance of the data will get better on your training data but will get worse"
            },
            {
                "timestamp": "11:30",
                "text": "when you look at new data because you're overfitting This is the classical view of the bias variance trade off And often this this particular gap happens when the number of parameters we have is uh essentially equal to the number of data points So this is how we normally think about it However in almost every single application in the literature sure um it turns out that this is not true In fact if you just train a very simple neural network and if anyone who's working on this I very much encourage you to actually check the number of parameters in your model You will find that often the number of parameters that you've used in your simple neural network way exceeds the number of data points like often by an order of magnitude Um And so what happens if you actually go beyond the interpolation limit right Like if you start adding more parameters"
            },
            {
                "timestamp": "12:00",
                "text": "The model should not work right False In fact it does better More parameters is actually even better You end up with this second decrease which is that when you have more parameters than data points the capacity of your model continues to improve You have even more things that you can use to model the data But you actually start going back down you actually do even better and most importantly you actually sort of asymptote to this result as you get an infinite number of parameters and the performance of your model is actually better than when you had a smaller number of parameters than the data points Right with"
            },
            {
                "timestamp": "14:00",
                "text": "really wild in my opinion. If someone said, hey I I have five data points and you give me a 200-order polynomial, I'd be like how do I even fit this? This is an ill-posed problem."
            },
            {
                "timestamp": "14:30",
                "text": "Turns out it works just fine. In fact, they should clearly be fitting with a, you know, 10,000-order polynomial. So this doesn't make sense, right? Like how is it that machine learning does not just memorize all of the data? It has you could just have one parameter memorize every single data point. Why doesn't it do this?"
            },
            {
                "timestamp": "14:30",
                "text": "Um in fact, you see something like this where this is sort of taken from this paper by Belkin et al. who are now at UCSD now. Um and what happens is if you have a model that has sort of the same number of features that you use to represent the data as data points, you get sort of this blue curve."
            },
            {
                "timestamp": "15:00",
                "text": "which fits all the data perfectly. Both of these models fit the data perfectly, right? That's the difference. But somehow when you add in more parameters, you go from 40 to 4,000, the function becomes smoother. So, even though you're allowing the function to have a lot more bumps and wiggles you actually get this smoothing out effect."
            },
            {
                "timestamp": "15:30",
                "text": "Which is really strange, right? Like why exactly does that make any sense at all? Um so what what does this mean, right? What is double descent doing, which is often what it's called in machine learning?"
            },
            {
                "timestamp": "16:00",
                "text": "The implication is that, not only do we do well, but in fact we should have as many parameters as possible, right? Our models should be bigger and more complicated. This is why, for example, chat GPT, I think in some of their more recent ones, it's like 70 billion you know, parameters in their model, I think now, they're on the hundreds of billions. Um so this is interesting and what it implies functionally speaking is that an overparameterized model with sort of more flexibility has to apply some sort of regularization on the function itself."
            },
            {
                "timestamp": "16:00",
                "text": "That's a different way of thinking of overfitting, but that's essentially what's going on with these very, very big machine learning models."
            },
            {
                "timestamp": "16:30",
                "text": "There's something happening inside the neural network that we haven't put in, like we haven't told it please fit like a simple function or something that's smooth. It knows that if it has a lot of parameters, the best thing it should do is fit the smoothest possible function with those parameters, which is pretty wild. Um so that's interesting because, instead of getting a super wiggly function when you have a bunch of parameters, in fact, you get one that is extremely well-behaved and now, it makes sense that if this is what happens, the more flexible you make your model, essentially the more you allow it to wiggle and bend, the better you can actually interpolate between your data."
            },
            {
                "timestamp": "16:30",
                "text": "So, what this implies is that ultimately, right, we're in this very interesting regime where if we look at this plot, we see that uh, essentially data is now the bottleneck, right? We can build a model, we can train a very big model."
            },
            {
                "timestamp": "17:00",
                "text": "And if you can interpolate between data in the most efficient way, you can think of this being like the principle of least action, for example, about how to go between two data points with the most flexible function that minimizes some sort of action. Um then you actually it seems like that's a good strategy for prediction, so something is happening inside. This also works for many function types. I don't want to caveat out exactly what these look like, but more or less all standard neural network stuff. This is totally fine. Um this is also pretty complex. So, I'll just point people to a paper. Um I think the one by Rocks and Mehta, I think 2022, um just to to sort of look into some of this from like a physics perspective."
            },
            {
                "timestamp": "17:00",
                "text": "So, I mean this computer, uh, essentially compute, but it's really essentially we just need to collect more data for our models to do better. They can continue to do better, and all of these models are doing better up to the limit of where we run out of images."
            },
            {
                "timestamp": "17:30",
                "text": "Okay. So what exactly is going on here? I want to take a step back and sort of think about what the heck neural networks are doing and why this emerges. And, I'll have a couple different view points here. To start with, by the way, I've not been paid to do this. Uh this is based on material from the Understanding Deep Learning book by  Simon Prince. Um this is absolutely the best book that's been published on this topic in a long time. Um it's pretty, it's online for free. You don't have to buy a copy. It's totally free. It has work exercises on like notebooks."
            },
            {
                "timestamp": "17:30",
                "text": "So, we can still do better. Um, so if we can just get more stuff, then things are great. But, this isn't feasible for us to ask this and scientists to classify like 10 times as more galaxies, right? Like, 107 million clicks is a lot of clicks."
            },
            {
                "timestamp": "18:00",
                "text": "But if you'd like to buy a physical copy, you can do that, which is what I did. Um okay. So what's happening here, right? Let's take a simple example of what a neural of a neural network which looks something like this. So often people draw neural networks, they draw a model that kind of looks like this, so and I'm just going to make this explicit, so just like Prince does. So really what's happening in this neural network is we have some input here."
            },
            {
                "timestamp": "18:00",
                "text": "You know, I don't think we can get a billion clicks in like a year, right? So, we need to to do something else, right? Um, so that just means that it pays to be clever. So, if you're thinking about a fixed data set size, you have a huge difference between choices of model, right? So you can think about this is having your model is learning something that effectively is having like 10 times as much data."
            },
            {
                "timestamp": "18:30",
                "text": "Um and this has essentially and a bias term. So that's sort of this one. So if you pass in something, you pass in the value, you also pass in this constant and what you're going to do is every uh time you're trying to go into a function, you're just going to be combining essentially a linear combination of all the inputs. So you're going to be essentially be looking at every edge on this graph, has a value, and there are two values that go into every prediction. It's the number that you multiply by one, and then the number you multiply by X, and that feeds into your function, which then does the same thing when trying to predict your final output."
            },
            {
                "timestamp": "18:30",
                "text": "It's a good bias, it's a good basis that you can pick. This is where really like, thinking hard does pay off, especially in astronomy where we don't expect that we can have infinite data, and eventually we will also saturate on the data."
            },
            {
                "timestamp": "19:00",
                "text": "So what does this look like in function space? I'm going to erase some of these things here. This is kind of what this means. So if we walk through this in order, what this means is that when we're going into uh sort of this first set of neurons what we're going to do is the first thing we do is we have sort of a slope and intercept. In this case 1 0 1 1. So that's these two guys over here."
            },
            {
                "timestamp": "19:00",
                "text": "Right? Same way that open AI and other models are saturating on data, too. You know, when you scrape the whole internet, what's left? Um, well, the YouTube videos that are now just transforming into Exactly, exactly! They're definitely not scraping those. Um, of course, of course. The CFO said so. Um, but, uh, but essentially we're in the same thing, right? So we need to think about how are we going to get more data, and it's exactly like that."
            },
            {
                "timestamp": "19:30",
                "text": "And so we're going to fit some line, right? So that's what we're doing, is we're taking our original input X, which could be at some value here, and we're going to transform it into some value on the Y-axis. We're doing that three times. So we have three different lines that we fit to the data. Then, we're going to go into the second one. We apply this function H to our data. So this function, it at least in this example, is what's called a rectified linear unit, which means it just is zero, uh if it is negative, and it's a line if it's positive. So that's what's happening here, is you see that it takes the original value and it truncates it at zero."
            },
            {
                "timestamp": "19:30",
                "text": "And also, if you think back to the first part of this, these models are essentially learning some representation that is really good for linear regression that can like, perform these types of tasks. But, that basis of learning is probably very generalizable, and this is why for example, foundation models work."
            },
            {
                "timestamp": "20:00",
                "text": "That's what's happening in all these cases. You can see it's just cutting out that negative part of the of the plot. This type of non-linear behavior is really important to make neural networks work because, if you just fit a bunch of lines and you multiply lines together, you get lines back. But once you start doing stuff like this, you're going to get other things back. And that's what's showing at the end, is that what you're doing at the very end is you're sort of multiplying each of these terms by a new constant."
            },
            {
                "timestamp": "20:00",
                "text": "Is that if you can learn essentially a bunch of features as in a very, very complicated set of features, you know, infinitely many in some cases that can uh, predict one particular task, it probably can do pretty well in another task that might be related. So, in terms of getting more data, right? Then, you have to think about we can get it from being multi-modal by adding in images, adding in spectra, adding in language. We can get it by doing new tasks that could use the same data in different ways, right? So by asking the model to do more things and look at the data in different ways, we're trying to extract more information from that."
            },
            {
                "timestamp": "20:30",
                "text": "And so you're going from say this one at the top to this one on the bottom and finally, to get your final prediction, we're going to take all of these guys down here and we're going to combine them together, and we get this. So what's happened here, right? What is our neural network doing? Is, it's essentially fit a bunch of stuff like a line. It then applies some non-linear transformation to the line. Those things get combined together at the end in a simple sort of linear model, and the output looks something like this."
            },
            {
                "timestamp": "20:30",
                "text": "So you might notice for example that this has particular line segments associated with it. Each line segment comes from an individual version of like these different models, right? And these different activation functions."
            },
            {
                "timestamp": "20:30",
                "text": "Um, and self-supervised. Right? In other words, can we get the model to just reproduce the data itself, at which point we don't need necessarily to have everything have a label that's been determined by a human or determined by some other theoretical model. The model can learn essentially just by itself to reproduce the types of data that it sees."
            },
            {
                "timestamp": "21:00",
                "text": "So what happens now if we think of this very simple framework, with three neurons with these types of activation functions that makes this function"
            },
            {
                "timestamp": "21:00",
                "text": "And so, in this sense, we have a way of essentially trying to say, look right now we're limited to a million, but if you could come in with LST coming with Sloan, we should actually be able to think about training and just everything that's there, and we should see big gains from that. So, that's why initiatives like Polymathic AI and AstroPyLe and other very large foundational model approaches, this is ultimately what's driving them."
            },
            {
                "timestamp": "21:00",
                "text": "So, let's make two sets of these. So, instead of just one, we'll have two different layers of neurons and we'll do the same exercise. So, we're going to now get something that looks like this."
            },
            {
                "timestamp": "21:30",
                "text": "Is that we expect to do better the more data we have. So, that's why we're always trying to get more data here, is that this the scaling, we're still at the point where we can collect, you know, an additional where, you know, a million images, we can absolutely go to a billion images, right? That's very doable. So, imagine this over three more orders of magnitude, about how much better that model is going to be."
            },
            {
                "timestamp": "21:30",
                "text": "So, before you remember we had sort of a simple function. Now, because we're applying two different steps here, we have essentially different combinations of these functions."
            },
            {
                "timestamp": "21:30",
                "text": "These things then sort of get added together and multiply together which give uh essentially going through like this."
            },
            {
                "timestamp": "21:30",
                "text": "And then finally, you get this output at the end when you combine everything together. So, that's interesting. So, by adding together these additional three neurons, we're adding essentially these new function approximations to our data. We're multiplying all of them together. And, what we get out is this very bumpy, wiggly line."
            },
            {
                "timestamp": "22:00",
                "text": "There's another uh, question from online, which is uh, how large uh, is large enough, when should you stop and when should you consider your data set size to be sufficient? Um, so essentially the one punch line here is that the data set size will continue to improve performance forever um, more or less."
            },
            {
                "timestamp": "22:00",
                "text": "Now, in fact this is essentially a one way to think about what neural networks are doing which is that really they are function approximators. And, depending on the exact function that you specify for these neurons you're going to get different behavior in the function that you get out over here."
            },
            {
                "timestamp": "22:30",
                "text": "Uh, at least for for lots of like, general self-supervised stuff. So, essentially it's really just diminishing returns. So, at what point do you hit some limit where at least functionally speaking it's just not worth it collecting that one data point, right? That fractional improvement in the model is too small to justify the extra computational costs, the extra in increase of the complexity of the model. Um, and uh, and sort of the task you're trying to do, it just takes longer for for you to do that."
            },
            {
                "timestamp": "22:30",
                "text": "Using this value, this rectified linear unit, for example, you essentially only have line segments, right? You either go up or down or you're flat. And, so, what you're doing is you're saying, I can approximate any function with some number of line segments which is true. In fact, you get an infinite number of line segments. You can approximate a function arbitrarily well. Um so, maybe it's not surprising that if you add more things then you can do better. Yes."
            },
            {
                "timestamp": "23:00",
                "text": "So, that particular diminishing return will be different for different science applications and different goals. Um, but that's sort of what we're what a good way to think about that. Same thing with the model itself, like sure having a more complicated model will do better, but again it's going to be diminishing returns."
            },
            {
                "timestamp": "23:00",
                "text": "And so Phil has been working on trying to apply some of these models to things like gyrochronology, which is modeling the spin down in stars over time."
            },
            {
                "timestamp": "23:00",
                "text": "Yeah, question. So, uh what do you mean when you add them here like what what's kind of like uh additional components mathematics speaking and why do you do you always use two choose um these layers to be like linear functions? Why does F2 be linear? Because linear usually it's not smooth you know like uh that you go to the infinitely small you know."
            },
            {
                "timestamp": "23:30",
                "text": "So, if you have sort of a fixed data set size, eventually your model is going to sort of saturate effectively and you're not really going to gain a lot by having it continue to be bigger and bigger. Um, and of course the bigger your model is, the more data you have, the more compute you're going to need, and that practically is probably what's going to limit us eventually. Um, I mean, if not already."
            },
            {
                "timestamp": "23:30",
                "text": "This is essentially a collection of data that he's assembled very painstakingly over the year of 30 different open clusters and young stellar associations."
            },
            {
                "timestamp": "23:30",
                "text": "Correct. So, so good question. So, the first is what's the math that's going on. If you kind of zoom in actually really close on here you can see what this looks like. So, this is essentially the, you know, linear combination in the first set. So, that's the math here is essentially what's happening at the uh the edges that sort of make up this prediction."
            },
            {
                "timestamp": "23:45",
                "text": "And you can see essentially in this plot on the right that as a function of age and color, you get this very very complicated spin-down behavior in stars that we don't quite understand."
            },
            {
                "timestamp": "23:58",
                "text": "And so Phil has... "
            },
            {
                "timestamp": "24:00",
                "text": "Can we stop here or should we uh, No, I can't I think we could not. Do you want to put Yeah. So, I'll sort of uh, skip ahead a bit I think, um, to sort of just comment a bit on on some other points and then I'll I'll wrap up in these first order of general questions."
            },
            {
                "timestamp": "24:00",
                "text": "Then if you go down, the sort of only math that happens between the layers is essentially a linear regression over the modified outputs. So, this one is essentially taking this linear combination of the inputs. So, uh in this case all the sizes, which are the individual input values and the edges, which are the outputs from the previous layer of the neural network and combines them and applies a new function to them. And then this last one is essentially just multiplying that by its own set of coefficients."
            },
            {
                "timestamp": "24:30",
                "text": "So, one of the other things that comes up in one of these cases is how well actually these functions behave or really another way to think about it is uncertainties, right? So a lot of times, I've talked about this function is if it's just one thing that kind of exists, right? A model learns one function, but there are actually many functions that could parameterize our data."
            },
            {
                "timestamp": "24:30",
                "text": "So, essentially, what you're really doing is every time you sort of you see a connection like this you should think oh I'm doing sort of linear regression. I'm fitting some combination of kind of weights, or biases, or coefficients to the data. So, I multiply it together, then I apply a function when I get to one of these neurons. And, then when you see things like this it just means I'm doing another thing linear regression. So, I'm going to take those new inputs, I'm running some new coefficients on them. I apply the function again. And then finally you run the linear regression for the last time."
            },
            {
                "timestamp": "25:00",
                "text": "And also, often our data have error bars, I haven't talked it all about measurement uncertainties, right? Um, which most people just ignore. Uh, but, they're ubiquitous, right? What we really care about is how well our stuff is going to do, and we know this is going to have an impact, right? You have noisy or lower signal-to-noise images, those are different populations of galaxies, different populations of stars than ones that higher signal to noise just because they're further away in the universe."
            },
            {
                "timestamp": "25:00",
                "text": "So, it's a big combination of doing linear regression, non-linear transformation, linear regression, non-linear transformation, um so that hopefully answers the first question. As for the second one, um that's a great question. I actually, so this is mainly for illustrative purposes, cuz it's easier to show these different line segments. In practice um what I'll actually get to very soon is that the function that you pick for these neurons governs a lot about the type of behavior that you expect that your function will have."
            },
            {
                "timestamp": "25:30",
                "text": "Um, so the real question is how much do we worry about this in theory and practice, right? That's ultimately the big concern. Um, and how do methods sort of deal with these and other strategies to incorporate them. So, one thing I want to just highlight is just thinking about this problem is that there are different types of uncertainties that sort of emerge from uh, from looking at this."
            },
            {
                "timestamp": "25:30",
                "text": "So, you can think about that it imposes structure or information on the function you can learn. If your activation function, for example, isn't sort of this super broken thing but is very smooth, it has very large dependencies, then you're going to learn something that also is potentially very smooth, doesn't change very quickly, has very long-term dependencies. Um and, that's actually a a very active part of uh of actually implementing some of these in practice is figuring out what's a good way to think about the structure in your data. Do you want it to be sort of smooth? Do you want it to change very quickly? Because maybe you expect that there's a particular selection effect that causes a very sharp break, for example, um or, do you want something in between. So, maybe you have some parts that are smooth, some parts that are not smooth."
            },
            {
                "timestamp": "26:00",
                "text": "I've I've talked about a lot of stuff."
            },
            {
                "timestamp": "26:00",
                "text": "So, we have an neural network, we can think about how uncertainties impacts various parts of this diagram. So, first let's think about our data, that's sort of what we're comfortable with, we can say, well, maybe we have some uncertainties on our observables, right? Some sigma x here. So, our input data to the model isn't quite perfectly known."
            },
            {
                "timestamp": "26:00",
                "text": "Making Like last question you asked. So, uh since every function mathematically can be represented as uh like uh a different sum of Fourier series."
            },
            {
                "timestamp": "26:01",
                "text": "Um you know hopefully it's been more optimistic than pessimistic for most of this."
            },
            {
                "timestamp": "26:02",
                "text": "Um and so all I really just want to say is that uh ultimately the big takeaway I want people to have from this is First oops So I mean it's actually works question mark Okay there we go Um that machine learning models like really represent a flexible representation or basis for data That I think the biggest thing I want to put uh put out there So whenever you use any machine learning model really what you're doing is picking a strategy for understanding your data and then you're just hoping that you can either scale into it or that the strategy is very efficient because ultimately we don't have infinite compute even our own brains cannot handle infinite compute Um let alone 70 billion uh parameters So I have enough trouble with three dimensions let alone like 10,000 So Um the second is that like we really are in a scaling driven era So I talked about these results for Mike uh but we can still expect a lot of things from being clever Uh I briefly talked about and I'm happy to go into a lot more detail on it if other questions or things in the break about that machine learning models are in some ways inherently probabilistic and you can actually really think about the modern era of doing simulation based inference and diffusion models and these types of things as being a natural extension of this underlying properties of these models essentially the space of all possible realizations of these models uh at a given time And finally which I didn't talk about Um we we hope that these will enable like a new class of domain specific uh formulation models that will drive have new like sort of different discoveries Just as a highlight these plots by the way is a discovery of essentially a bunch of different uh carbon enhanced metal poor stars that came from a completely data driven model Um that just popped out of the data and look beautiful and we're following up on them now So this is a good example of actually going through and putting everything I said into practice and trying to find some new exciting stuff So if you have any questions on this or more things definitely feel free to talk to me at any time Thanks"
            },
            {
                "timestamp": "26:30",
                "text": "Thanks That was a perfect talk to start the week Absolutely perfect Hey Yeah Um so I think uh we have a few more minutes for questions but I just wanted to make one comment So we have the contributed talks later and I think there's probably a good recommendation to come up if you do have a talk in that session come up during the break and check your talk make sure it works and you know all the slides and stuff like that Um so anyways let's open up for a few more questions Um feel free"
            },
            {
                "timestamp": "26:30",
                "text": "So, maybe we want our model to condition or marginalize over that uncertainty when it sort of propagates through. And then, you know, instead of just a point, we sort of are feeding in some distribution of possible values that our model could "
            },
            {
                "timestamp": "26:30",
                "text": "Yeah. Can we use something like that here, to like you know, because anything can be fit into Fourier you know. Correct. So, so actually, that's uh so, there's something called the universal approximation theorem, which says that all of the standard uh activation functions that are used in neural networks have the same proof. So, they can be used essentially infinite number of them, can reproduce any function. You absolutely can attempt to use something like a Fourier series here. Um there there is some work that's done versions of this where the activation functions are more like signs, or cosines, or some versions of wavelets, or things like that. Um so, all of that logic actually holds very well."
            },
            {
                "timestamp": "26:58",
                "text": "Okay thank you for the talk Uh I was just wondering when you say add more parameters gets your network better Is there And you And you mentioned about being clever about it but isn't it just simply make it wider make it deeper make more dimensions or reduction Is this What's the best way to If you add more network make it have more parameters That's uh Oh gosh Uh the answer is that uh is yes but really it's it's very much a uh more of an art than a science So when you make uh if you think about sort of these like each each function is being some sort of representation or some bases you can use Every time you make a layer broader you're sort of increasing the representation of that particular feature in your model So you know if you make it broader you're adding another uh you know sort of segments to the if you're adding a Relu or you're adding some more twists and turns Um but every time you make a sort of a second layer deeper and you're multiplying sort of all those functions together you're making them much more complicated So in general uh the reason why people If you take the same amount of parameters and you make uh a model that's just really wide versus one that is sort of some mixture between wide and deep That one is going to be much more effective just because the product of all those different functions will create a much more complicated space much more quickly So in general the the advice I have is you want to sort of make sure that every layer has enough expressivity that you sort of feel that you know yes I can prob it's it's a good thing I think I can summarize my data in like this many parameters kind of effectively Um and then if you have extra space add another layer and you'll get sort of a whole new range of of possible function realizations So generally that's that's my uh my recommendation"
            },
            {
                "timestamp": "27:00",
                "text": "Okay. Great questions. So yeah. All right. So, I just wanted to follow up, this I was I was just wondering, cuz I feel now that I think this is the clearest explanation I've seen as to why we use ReLU for everything. Uh and I have to say, yeah, it's it's it's a very clear, but it's it's sort of related to what I was just asking, because uh uh why would you then use anything that is not ReLU even the fact that in principle, you could just approximate anything with this. Um is it a matter of uh accuracy in the results? What about the fact that ReLU is not differentiable, right, at the point where it goes from zero into linear and what about all the cities?"
            },
            {
                "timestamp": "27:30",
                "text": "Right. So, uh for one of those, I'll say that I also really dislike the fact that it's not differentiable, but it does actually make things easier for some theoretical stuff. For the second point, I'll say uh hold off for a little bit. We'll actually get to this uh in probably maybe 10 or 15 minutes."
            },
            {
                "timestamp": "27:30",
                "text": "Okay. So, those are great questions and hopefully that was very helpful to everyone else in the audience and online. Um but, the idea now I think makes a bit more sense, right? That somehow if we add up all of these different line segments together, if we have some you know, some small basis, we sort of add more and more terms or breaking up our function into smaller and smaller parts or projecting it into some different domain then it makes sense that if you have for example you can see these are ReLU features. So, if you have 40 of them you get these sort of very sharp line segments. But, when you have a lot of them you can sort of stitch them all together, you get this very smoothly varying thing. If you zoomed in enough you would see that there are also line segments. But, in this case, you have enough of them that that the function becomes sort of more locally smooth."
            },
            {
                "timestamp": "28:00",
                "text": "This is interesting."
            },
            {
                "timestamp": "28:00",
                "text": "So"
            },
            {
                "timestamp": "28:01",
                "text": "Um,"
            },
            {
                "timestamp": "28:02",
                "text": "So, why exactly does this happen, right? I want to provide three different perspectives, all of which turned out to be somewhat correct."
            },
            {
                "timestamp": "28:08",
                "text": "Um, on what's going on here. So, the first one you might think is stochasticity. This is just a training thing, right?"
            },
            {
                "timestamp": "28:14",
                "text": "So often for completely computational reasons, when we train a model, we don't actually look at all the data at once."
            },
            {
                "timestamp": "28:20",
                "text": "We only look at a small segment of the data."
            },
            {
                "timestamp": "28:22",
                "text": "And so this started out as a very computationally focused thing for training neural networks, but it actually has some really, really interesting, uh, consequences."
            },
            {
                "timestamp": "28:30",
                "text": "So for example, let's consider this model like, like here. Right, what happens when we have a super complicated, uh, you know, neural network with all these different features?"
            },
            {
                "timestamp": "28:40",
                "text": "It's going to start out by maybe looking something like this, right? It's going to have all these random stuff everywhere. It's going to be a mess."
            },
            {
                "timestamp": "28:47",
                "text": "But, when we start training what we're going to do is we're going to take just a subset of those points. You know maybe just these three."
            },
            {
                "timestamp": "28:55",
                "text": "And, we'll try and figure out how to predict those three points. And the next time we'll see these combination of points and we'll do better there, then we'll see that combination of points."
            },
            {
                "timestamp": "29:02",
                "text": "You know, every time it's only looking at a small subset of the data."
            },
            {
                "timestamp": "29:07",
                "text": "And if there's enough data points, which usually there are, any random batch is essentially guaranteed to be unique."
            },
            {
                "timestamp": "29:13",
                "text": "So, you're never going to look at exactly the same subset of data twice."
            },
            {
                "timestamp": "29:15",
                "text": "Thank you"
            },
            {
                "timestamp": "29:18",
                "text": "You're always going to be looking at something slightly different. And so, now every time you tweak the model you're trying to get to predict a slightly different thing it's never seen before."
            },
            {
                "timestamp": "29:21",
                "text": "Thank you for the talk Uh since we are training the data based upon the known models which we have what else the new images or the new data which we given Uh it doesn't have any or it is something new Uh the physics involved is new then how do you trust the because that our model which we are machine learning model which we create it may discard it as something uh uh wrong or something not correct then how do you trust the process at the end because the human inte uh human involved men will be required at that"
            },
            {
                "timestamp": "29:26",
                "text": "It's still part of the big data set. So in aggregate it's seen the whole model. But, every time you're trying to improve the model, you're always looking at a different subset of the data."
            },
            {
                "timestamp": "29:35",
                "text": "So, if you now think about what's happening, right? Every time you get a different subset, the model overfits in a slightly different way."
            },
            {
                "timestamp": "29:42",
                "text": "But if you do this enough times you're going to average down on that overfitting, right? Sometimes you'll get kind of pulled up a little bit high here, then you'll get pulled a little low, and then over time essentially these things will dampen out into sort of the most efficient, uh, regularized, approximator."
            },
            {
                "timestamp": "29:55",
                "text": "Yeah So there there that's a great question So I didn't go into this me I think a lot about this problem actually Um so I'll give sort of a quick answer and a more detailed one So the quick answer is that when you have a new image that comes in one way that naively you could say oh is this image different is you just compare it to the set of images you have That's sort of like a nearest neighbor type thing but you can also imagine doing the same as it processes through the the the network right as you sort of change that that data into a new set of features a new set of realizations it's doing a bunch of operations set image which may be actually turn out to work pretty well like edge detection So it actually can look a lot like a lot of things in your data set but it could also look very different So if you think about doing sort of deep nearest neighbors which is looking at the trajectory that your particular model takes or your particular data takes through the network and comparing that to the trajectories of sort of typical data that you have You can see whether those actually differ by a lot and if there's a large difference between them you can flag that particular object as potentially being anomalous or uh or out of distribution or bad uh predictions"
            },
            {
                "timestamp": "30:00",
                "text": "This is often called like sort of implicit, uh, or inductive biases or implicit regularization."
            },
            {
                "timestamp": "30:07",
                "text": "But, the idea is that actually because you're always seeing a different subset, the model never can quite memorize the data the way that you think."
            },
            {
                "timestamp": "30:13",
                "text": "It always is going to be doing a little bit more of this."
            },
            {
                "timestamp": "30:17",
                "text": "So over time, the expectation which is what you'll get when you train over many, many different iterations, will eventually converge to sort of the best, uh, approximator that you can get."
            },
            {
                "timestamp": "30:25",
                "text": "But, this requires a lot of time. Right? So you have to make sure to see a lot of the data. And again, the key thing here that I want to emphasize to people is that, you will not see any differences on the training data, in theory."
            },
            {
                "timestamp": "30:37",
                "text": "Right? Because the training data is perfectly approximated by both sets of models."
            },
            {
                "timestamp": "30:42",
                "text": "The difference is going to be on whatever validation or test data you have. So even though you're going to hit zero and your model says I fit the data perfectly, once there's a change, every time you see a different stochastic realization, the model's going to pull a different direction."
            },
            {
                "timestamp": "30:55",
                "text": "In practice you don't get there. Right? In practice you're always going to be tweaking just a little bit more, your model's always going to be trying to go a little up, a little down and eventually you'll sort of average out to the right behavior."
            },
            {
                "timestamp": "31:00",
                "text": "In general though it's uh there are some cases where you can actually extrapolate from these types of models because it's learned very good basis representations that can be applied to other things"
            },
            {
                "timestamp": "31:05",
                "text": "That's, that's one way of looking at this."
            },
            {
                "timestamp": "31:08",
                "text": "So, I think this is very appealing."
            },
            {
                "timestamp": "31:10",
                "text": "I like this view a lot. Um, turns out this is actually not the most in important thing. Um,"
            },
            {
                "timestamp": "31:16",
                "text": "So, what else is going on, right?"
            },
            {
                "timestamp": "31:19",
                "text": "Okay. Second thing is we can think of feature space. So, deep learning, right? What's happening here. Um, we can instead of thinking in terms of the function that we're sort of fitting with our neural network. Let's think about what it's doing to the data."
            },
            {
                "timestamp": "31:30",
                "text": "So it's not necessarily guarantee that uh you know things will fail out right but it's always good to be sure So I'd say that's sort of my uh so the longer answer is to you know if you use these types of deep neural neighbor strategies or looking at sort of these trajectories or sort of the distribution of trajectories that uh objects take through your network um that can be a really effective way to tell am I seeing something really different or not and then flagging it before it gets passed to to uh you know to do some downstream science"
            },
            {
                "timestamp": "31:33",
                "text": "So, what it's really doing is applying some complicated function, which I'll call phi, and that function's transforming the original data into something else. And, this transformation is actually really commonly used for a lot of different problems."
            },
            {
                "timestamp": "31:48",
                "text": "For example, let's think of a classification problem."
            },
            {
                "timestamp": "31:51",
                "text": "This is in one D. I pulled this from this nice blog post, um, by, uh, Drew Bemis."
            },
            {
                "timestamp": "31:56",
                "text": "So, let's imagine we have a one dimensional function, you're trying to classify between sort of the orange and the blue, and you get something like this on the left."
            },
            {
                "timestamp": "32:00",
                "text": "So you can do very effective automatic outlier detection I had a good time today I may have one more question Um break off by the way uh So I think uh the unique thing in astronomy is we often train with simulated data uh in which case you technically have infinite training data"
            },
            {
                "timestamp": "32:04",
                "text": "Well, that's, you know all you can do. Let's just assume that all we know how to do is we know how to fit a line, right?"
            },
            {
                "timestamp": "32:10",
                "text": "I'm a statistics person, that's all I know how to do. So, I look at that. I can't really fit a line. Right? Like, how can you separate those by fitting a line. The best one you could do would be right in the middle, and that doesn't really help you."
            },
            {
                "timestamp": "32:20",
                "text": "But, you can augment the data by adding a new feature."
            },
            {
                "timestamp": "32:25",
                "text": "So, now we're going to add this new thing called X2 which is just going to be X1 squared."
            },
            {
                "timestamp": "32:30",
                "text": "Yeah Well your limit is then uh computational not training data So do we just need as many parameters as you could possibly fit in that case since you have infinite data or what should you do Right So two answers one is that in the simul so the the big problem with simulated data is actually domain adaptation So the difference between your simulations and reality Your simulations have some un fundamental set of parameters that you use to characterize the simulations and so you actually know to some level like what the complexity of that is You know not of course if your simulation is very complicated But in theory like there it is finite somewhere Um the the problem is then not that you want to describe the simulations perfectly at which point you would want to just keep adding more and more parameters until you get you know better and better But that when you go from the simulations to the real data that it's actually looking at a totally different population of things"
            },
            {
                "timestamp": "32:31",
                "text": "So, that seems reasonable. This is what's known as a polynomial feature. So, we're just going to take the input data, we're going to square it, and that adds this. So, now we have two features, X1 and X2."
            },
            {
                "timestamp": "32:42",
                "text": "They're not new, right? X2 is, is essentially just X1 squared. But now notice what happens."
            },
            {
                "timestamp": "32:47",
                "text": "We get this quadratic, and now we can draw this line in this quadratic space that perfectly separates the two classes."
            },
            {
                "timestamp": "32:56",
                "text": "We've learned nothing new. This was all available in the data."
            },
            {
                "timestamp": "33:00",
                "text": "That is actually I think more of the problem when looking at simulations And it's why uh my feeling is that you don't need your model to describe simulations perfectly because sometimes you don't the things that are in the simulations are not the same as the observations and so you don't need that level of detail Um so you it's worth spending more effort on the other problem which is how do I guarantee some mapping say some optimal transport from the distribution I have with my simulated data to the actual distribution with my observed data with sort of the minimum distance between those two um so that I can hopefully get some something close um rather than just predicting completely different stuff Hopefully that answers your question"
            },
            {
                "timestamp": "33:01",
                "text": "All we've done is learned a transformation that essentially takes something that was nonlinear and puts it in a higher dimensional space where it's linear."
            },
            {
                "timestamp": "33:12",
                "text": "This is very, very powerful."
            },
            {
                "timestamp": "33:14",
                "text": "You can use this all the time. So, this is the same thing now applied to a two dimensional, uh, classification problem where you want to select all the points in a circle."
            },
            {
                "timestamp": "33:23",
                "text": "You do the same sort of transformation, you get something looks like this, and voila, you can now draw like a plane through that separating those things out."
            },
            {
                "timestamp": "33:30",
                "text": "Great Great Excellent question"
            },
            {
                "timestamp": "33:31",
                "text": "So in general, right?"
            },
            {
                "timestamp": "33:34",
                "text": "What's happening is that you can think of this in two ways, a complicated function in our input space, is really just a line or some sort of hyperplane in some higher dimensional space that's just getting mapped back to our original inputs."
            },
            {
                "timestamp": "33:37",
                "text": ""
            },
            {
                "timestamp": "33:48",
                "text": "So, in the higher dimensional space it is just a plane. But like we can think about it also as a complicated function."
            },
            {
                "timestamp": "33:56",
                "text": "So, this essentially is what our transformation phi is doing."
            },
            {
                "timestamp": "34:01",
                "text": "Now, this should feel very familiar, because it seems a lot like a neural network."
            },
            {
                "timestamp": "34:07",
                "text": "Right? In the sense that in our neural network I just described that we have sort of this linear regression, and then we have some, you know, nonlinear transformation, and then we do linear regression again, and then nonlinear transformation and then linear regression at the end."
            },
            {
                "timestamp": "34:22",
                "text": "So, but really like it's this part at the end that's important, right? Like, we're running a linear regression at the very end, or if it's classification, we do linear regression and then some like logistic function."
            },
            {
                "timestamp": "34:34",
                "text": "Um,"
            },
            {
                "timestamp": "34:35",
                "text": "So, what does that mean, right? It means that literally everything prior to this point, this is all feature or representation learning."
            },
            {
                "timestamp": "34:50",
                "text": "What this means is that, really what we can think of neural networks as doing is doing some crazy complicated stuff that can fit the space of all possible functions."
            },
            {
                "timestamp": "34:58",
                "text": "And really all it's trying to do is p p p p p p"
            },
            {
                "timestamp": "35:00",
                "text": "get our data into some feature space where we can run linear regression. That's what we're doing. We're using so much compute power to run linear regression. That's really it, right? That's it. That's all This is This is extremely important, right? This is one way of thinking about what's happening that also is very intuitive and also has a lot of other benefits."
            },
            {
                "timestamp": "35:30",
                "text": "So this is often called the kernel trick because it turns out you don't need to apply {Phi} directly to make predictions, but only the inner product {K}(x, x') = {Phi}(x), {Phi}(x')."
            },
            {
                "timestamp": "36:00",
                "text": "So people have used other methods before like support vector machines or sort of other types of regression or if you've heard of kernel ridge regression or these types of things. This is called the kernel trick because it turns out that mathematically you don't actually ever need to compute {Phi} like you don't actually need to to know what this transformation is. All you need to be able to do is compute the inner product of sort of two different points that essentially x and x' um and that essentially is enough for you to do predictions."
            },
            {
                "timestamp": "36:30",
                "text": "So kernel regression looks like this. Um the definition of the kernel is {K} I use the same notation here as I have elsewhere um and so all that we're doing here is just This is how you do a prediction. You want to get a prediction for a new point you get the You take the x values you you times it by the you take the kernel of it with your training set. You then take this kernel of the training set with itself and you take the inverse and multiply it by y That's just the math thing. The important thing you'll notice that the only thing that shows up in this equation is the kernel."
            },
            {
                "timestamp": "37:00",
                "text": "So in theory I could have done this particular transformation, but all that I actually need to do is understand the inner product of what this does. And this is important because a kernel has a certain way of thinking about it, right? People talk about kernel density estimation, people think about kernels with respect to lots of other things. This sets essentially what we might think of as the covariance structure or the general structure of your function."
            },
            {
                "timestamp": "37:30",
                "text": "So now we're starting to see something interesting, right? Like we talked about Oh choosing activation functions it it sets some sort of structure to your reconstruction. Now we sort of see that under this different perspective under this kernel trick perspective, we also to have something that's based on the structure of the of the space."
            },
            {
                "timestamp": "38:00",
                "text": "Yes um This uh reasoning suggests that you always want to increase the dimensionality of your input space."
            },
            {
                "timestamp": "38:30",
                "text": "Yes indeed it does. But often in astronomy we try to reduce the dimensionality of many things. Yes. Under that works. So it really depends on what you're thinking of here. I'm actually That's a great question and we'll we'll get to that actually. So I'm about to take the limit of infinity um but So but in practice it really comes down to function-wise it's great to sort of increase this and have many more parameters, but eventually if you want it to be interpretable or you want your data to be say generative you want to take some very flexible function and then ask if I only had three parameters or six parameters you know what would these be? I know then astronomy things like redshift matter, things like metallicity matter um things like the luminosity matter so probably if I force it to do its best with only three things it should it should learn something that is closer to my physical understanding of the problem."
            },
            {
                "timestamp": "39:00",
                "text": "So you're going to lose information. That's what this is saying is that you know naturally if you have infinitely many parameters, you'll do perfect, but uh that's sort of the way that I like to think about it. So often it's good to be essentially expansive when you're starting and then eventually when you're ready for it to to learn some stuff, you can then contract and really try and get it to force it to to use the minimum number of parameters you like. But you don't need to. That's essentially my big uh big thing here."
            },
            {
                "timestamp": "39:30",
                "text": "All right, great questions, everybody. Okay, I promise, by the way, this will all pay off at some uh at the end where we're sort of introducing all these deep concepts that's going to thread through the rest of like the actual applications. So last but not least, I'm going to go back to functions. So now that we know essentially that there's really this the thing that matters is the structure of the functions that we learn of our neural network. That structure is going to be determined by the activation functions we use and the architecture, the way that we set up the neural network to actually do things. We can now say well why do we need a neural network at all? Why can't we just using infinite mapping essentially. Um and you might think this is kind of crazy, right? You're going to take a finite number of data points and generate an infinite number of features. This actually happens all the time. So for example in KDE or kernel density estimation, we often will take you know some data point data that is at some location and then turn it into some function that exists everywhere. And then we add those functions together and then evaluate them again. But the idea is that we actually do this pretty frequently, right? Or if you do a Fourier transform, you've really evaluated your data on the space of all possible frequencies, right? It has infinitely many parameters too. Um there are many cases where actually you do this all the time and don't think about it where you've really have some infinite dimensional function space that you've applied to your data to transform it from one input to another."
            },
            {
                "timestamp": "40:30",
                "text": "And so we can think of it again that we're doing something like this and so what happens if we take our neural network to have infinitely many parameters. In this sense, we're going to take our original thing and we're going to say what if we have not just three, but we go all the way out and each of these goes to like infinity. Now based on all this leading stuff you've actually get a kernel. It's often called the neural tangent kernel or the NTK. And so this essentially says if I turn my neural network into this infinitely wide thing with as many parameters as possible, it will actually give me a space of functions that will have some structure. And in fact the structure is going to be based on on the activation functions I use and the structure of my neural network. Uh and you can actually make predictions the same way. You just take the kernel you do the same things. It's the same notation as before."
            },
            {
                "timestamp": "41:00",
                "text": "Okay, I don't want to go through the math um I just want to say if you're interested there are some great blogs uh by Borealis AI, which is also uh run by by Simon um and it looks like this. You take the outer product of the derivative through your entire network uh you can derive analytical expressions in the infinite dimensional limit. I don't want to go into this um anyway it exists. You can find code online um it's very difficult actually to compute these um in in general. So the ReLU is used particularly because it's easy."
            },
            {
                "timestamp": "41:30",
                "text": "Okay so if we go back to this though, we say Okay, now we've gone crazy, right? We started with some neural network with three neurons and now we have infinitely many neurons and but we know that actually our model should do better, right? the more we've added and that's because we've essentially allowed it to fit the most flexible function possible. And you can see that sort of you know visually in this example. So here is just an input and output that we're trying to match. Ignore sort of the lines, those are based on like different training iterations in some time dynamics. Um the main thing to focus on is the solid green one, which shows at the end of training, sort of what does this function do? And you can see that as you add p p p p p p p"
            },
            {
                "timestamp": "42:00",
                "text": "essentially more and more parameters here that eventually it gets better and better and closer and closer to the interpolation limit where it fits all the data perfectly."
            },
            {
                "timestamp": "42:00",
                "text": "You can see how at the end of training, sort of the NTK function becomes the real thing in the sense of like, you have an infinite number of neurons, so that actually that function at the end of training becomes the actual function."
            },
            {
                "timestamp": "42:30",
                "text": "Now we can take the same thing and say what happens if we now go way into this regime."
            },
            {
                "timestamp": "42:45",
                "text": "So we start here It's just like our previous one, we get better eventually we interpolate everything and if we go all the way to infinity that's what's shown here, we use this equation, we go to infinity, we get this."
            },
            {
                "timestamp": "43:00",
                "text": "Which makes sense like you see it kind of looks a bit more like line segments when you go to infinity that makes that does make sense given that when you use a re-LU activation function it is kind of choppy it is kind of line segmented."
            },
            {
                "timestamp": "43:30",
                "text": "So maybe we're doing some sort of linear interpolation of the data um in very high dimensions and that's probably a good strategy in general."
            },
            {
                "timestamp": "44:00",
                "text": "So interesting, right so in other words we now know why this modern interpolating regime actually works and in fact we've taken the limit as the number of parameters in our model approaches infinity."
            },
            {
                "timestamp": "44:30",
                "text": "Um again, we have a finite number of data points but in theory our model has has become infinitely complex."
            },
            {
                "timestamp": "45:00",
                "text": "Okay this gets back to uh I forgot to ask this question about picking a really good basis."
            },
            {
                "timestamp": "45:15",
                "text": "Um So this ultimately right, is all about the properties that you want So if you think about this way of doing neural networks your activation function is setting the structure that you want your big functions are going to learn to have."
            },
            {
                "timestamp": "45:30",
                "text": "You want it to be smooth, pick something like a sigmoid or a logistic You want it to be rough or very choppy, re-LU is great Want to have global properties, maybe you can use signs or cosines or some dampened versions of those."
            },
            {
                "timestamp": "45:45",
                "text": "You want it to be very local You want to have a very isolated term You want to be stiff and not change very much You want to be very flexible These are all going to be considerations for you know what you want And of course you can mix and match right there's no reason that you have to only stick to one type of function you can have tons of different combinations of functions as you make your network deeper."
            },
            {
                "timestamp": "46:00",
                "text": "Um so fundamentally what this means is that in deep learning, right you're really trying to generate some properties of your function and fit it to the data."
            },
            {
                "timestamp": "46:15",
                "text": "Um and you just are ultimately trying to interpolate between the data and hoping that the function you pick does a good job."
            },
            {
                "timestamp": "46:30",
                "text": "Now the limit of this is sort of what I like to to call symbolic regression which is work that's led by Miles Cranmer, who's uh now at the University of Cambridge."
            },
            {
                "timestamp": "46:45",
                "text": "Um and I hope he used this package before This essentially says, rather than trying to fit like weird complicated functions why don't I just fit like a polynomial or an exponential or a log right like I can use addition subtraction exponentiation that's probably a good thing Um in fact Miles has done some great work where he's like rediscovering Newton's laws and like shown that you can really learn real physics from from neural networks this way."
            },
            {
                "timestamp": "47:00",
                "text": "And you might say, cool Well, why does this work And we can use the same logic that we've just had to figure out why this is an effective strategy right In other words, symbolic equations are really good basis for modeling real-world phenomena by construction, we learned math and developed math because it described the world around us."
            },
            {
                "timestamp": "47:30",
                "text": "Right you know we didn't just sort of pick some random stuff out of a hat and say, oh, let's do this operation addition subtraction exponentiation logarithms like all of the ways that we process and think about data that we do physics that was discovered through the real world So it makes sense that actually it's a really good guess to for describing how behavior and these types of very complicated systems will behave too."
            },
            {
                "timestamp": "48:00",
                "text": "So there's nothing better or worse about symbolic regression for example than using a neural network It's just a different choice of basis and a different strategy This just turned out to also be very effective um if much harder to explore."
            },
            {
                "timestamp": "48:30",
                "text": "And I just want to say this is all uh so far has been mostly kind of theoretical, but I now want to show that this this will drive a bunch of concerns about how exactly this is going to work in practice And I want to show first because this was a great example actually from my uh from a summer student I was working with."
            },
            {
                "timestamp": "48:45",
                "text": "Where, after he learned about this, I think 2 weeks ago he was like, I want to check if this is true and so he trained this model, I don't want to get into it, but uh trying to reproduce Gaia XP Spectra So that's him uh that's Alex, um and he essentially looked at data from Gaia XP uh which is uh from Gaia DR3 This is showing essentially the behavior, change in the spectrum uh that you get from the BPR uh Spectra as a function of temperature for stars so you can see the range and possible behavior that this can have And he had a machine learning model that learned to predict essentially what this look like."
            },
            {
                "timestamp": "49:00",
                "text": "Just to last follow-up like like for example in that plot. Yeah. Is there a physical reason why we should prefer the higher parameter model over the 1000 parameters?"
            },
            {
                "timestamp": "49:00",
                "text": "And lo and behold uh he showed exactly this that as you increase the number of parameters, essentially the complexity of the model, you get this decrease in the training and you also get this sort of two double descent behavior And in fact this peak here you can prove, and we find is essentially equal to the number of training objects that we have, which in this case is like 30,000 or so."
            },
            {
                "timestamp": "49:30",
                "text": "So in this case no, there's not a physical reason. This is all about functional capacity. So in other words, turns out that spectra are very complicated and so having a model that can describe them just requires essentially having a very very flexible structure."
            },
            {
                "timestamp": "49:30",
                "text": "So we actually show this exists in real astronomical data, so it's not just a theoretical thing It shows up all the time Uh you can try it in your own cases but this is very fun Um so I thought I'd highlight some of his work because uh it was really exciting when I saw this last week."
            },
            {
                "timestamp": "50:00",
                "text": "So, ultimately actually I'll get back to this. There there is a way that we can interpret this physically. But this is just more saying that turn out spectra are complicated. You need a lot of parameters."
            },
            {
                "timestamp": "50:30",
                "text": "Uh pretty much the same question as the last one because here you clearly see that your point at uh south parameters is pretty much have the same testing loss that in your uh much greater one of the end of the red descent."
            },
            {
                "timestamp": "51:00",
                "text": "So, the only difference is your computational cost. Sure, but don't you think you could have spent it enhancing the architecture for example with the 1000 parameters by optimizing better simpler neural uh autoencoder than spending it on training with a million or whatever the lot people."
            },
            {
                "timestamp": "51:30",
                "text": "Oh yeah. I mean, of course right like uh you can always do better. I'll actually get back to how you can win by being clever in a little bit. I wouldn't be too hard on Alex. He's a second-year student, so uh, you know, this is this was more of a project that he did completely just on his own uh to check whether this is true."
            },
            {
                "timestamp": "52:00",
                "text": "Yeah. Uh, pretty much the same question as the last one because here you clearly see that your point at uh south parameters is pretty much have the same testing loss that in your uh much greater one of the end of the red descent."
            },
            {
                "timestamp": "52:30",
                "text": "So, the only difference is your computational cost. Sure, but don't you think you could have spent it enhancing the architecture for example with the 1000 parameters by optimizing better simpler neural uh autoencoder than spending it on training with a million or whatever the lot people."
            },
            {
                "timestamp": "53:00",
                "text": "Yeah. It just makes it really easy for the model to essentially memorize the number of data points. Um so if you essentially if you think of the sort of easiest solution that gives you good predictions. Um it right around there when you have you know the same number of parameters as data points, sort of the best strategy tends to be just memorize the data as much as possible."
            },
            {
                "timestamp": "53:30",
                "text": "If you have way too many and you're also looking at random batches every time it actually becomes harder to do that. Um and again the function space view suggests that you sort of get these very very quickly functions and so uh during training that that essentially happens less. But that's it. I think more or less the general understanding. There's actually quite a bit of work on on double descent. So if you Google this around or I guess now you can ask ChatGPT. Uh there much uh and uh hopefully you can find some references. But it's a very active field of research."
            },
            {
                "timestamp": "54:00",
                "text": "Okay. So I'll sort of close. This has been great, so I think this'll end up being a lot more pedagogical than necessary application, but that's great cuz I'm I'm very happy everyone's asking questions. What this really means about what people in machine learning are thinking about and how this applies to astronomy is sort of three different words."
            },
            {
                "timestamp": "54:30",
                "text": "One is capacity. So, this is me one way to think about machine learning right and deep learning in particular. It's all about efficient basis representations. And this holds true for every method right. If you're using a random forest, you're using a support vector machine or k-nearest neighbors, any approach you have essentially is in the same boat because they all either use the kernel trick or all use some other ways, which means they're all learning some sort of function."
            },
            {
                "timestamp": "55:00",
                "text": "So, all that you want to do is really think hard about what's the strategy you have to tackle your problem. Do you want your function to be smooth? Do you want it to be flexible? Do you want to have local behavior? Do you want to have some very defined decision boundary that doesn't change. Um this is really going to be the thing that drives lots of this stuff and this is all called capacity right. Um and so the idea is that sure, any model can get a perfect fit to the data with an infinite number of data points. But different strategies are going to get there at different speeds."
            },
            {
                "timestamp": "55:30",
                "text": "So, if you pick a, if you're smarter about it you'll pick a better basis you'll do a better job. For example, right, if we think about sort of this feature space view or function space view and we want to say reproduce you know, sawtooth like level behavior, we could do that with the Fourier series but actually it's very inefficient. Um you get tons and tons of ringing effects because of the non-locality of the of the Fourier series and the sine and cosine terms. But if you switch it to be something that's a bit more local, you'll do much faster much more quickly. Both of them will work with infinite number parameters, but we really care about how quickly you can get there with a finite number of parameters."
            },
            {
                "timestamp": "55:59",
                "text": "The second is scaling. Right? In other words, if we have an infinite dimensional space that could always fit every single data points then the more data we have the better we're going to perform. Right? In other words, if we're interpolating between p p p p p p"
            },
            {
                "timestamp": "56:00",
                "text": "We're feeding as much data as possible, the more training examples we can provide, the more flexible a function can be, the more it can change locally, the more it can change sort of globally, and that means that we always want to give it more data. So more data for a more complicated infinitely complicated model, always will, will be better in general in theory. "
            },
            {
                "timestamp": "56:30",
                "text": "Um, the final thing is compute. Of course to do all of this you need a lot of computing resources. Um, and also training actually does play a role. I sort of opened with this idea and intuitive idea that stochastic gradient descent plays this fundamental role, and it does actually. So there's a bunch of empirical results that actually show that even if you use this infinitely wide, you know, representation of a neural network, it does worse than a finite one. "
            },
            {
                "timestamp": "57:00",
                "text": "In other words, the finite thing that learns some very you know complicated stuff and trains the stochastic gradient descent routinely outperforms the infinite parameter version of the model. Right? And people don't exactly know why that is, but it's, so I, I'll skip that. You can read some additional stuff in like Arora, Lee, um, also Smith. Like, there are ideas behind why this happens and sort of some bits and pieces of why this is occurring, but clearly something else is going on, right? Something about training itself, about how exactly these architectures learned, is doing extra stuff, then essentially just fitting every possible function or the most complicated function possible. "
            },
            {
                "timestamp": "57:30",
                "text": "So, you know, this sort of view of how we get to this smooth black curve instead of this smooth blue curve, we still don't quite understand, but hopefully today you will have a bit of a better understanding of why people are really interested in this. "
            },
            {
                "timestamp": "58:00",
                "text": "Sorry, there's a question from the chat. Oh, perfect. Um, this was actually from a bit earlier, I think, but it's still relevant. Um, so, if you had asked online, I still don't understand the meaning of having larger parameters than the data points. Um, because there are particular, um, uh, data sets in thousands of numbers. So they're asking again about the larger number of parameters than data points. "
            },
            {
                "timestamp": "58:30",
                "text": "Yeah, so in this case, the, the idea is that really, the parameters are just telling you what function you can fit. And so even if you have sort of infinitely many parameters in your neural network, what you're really saying is that with a finite number of data points you it specifies the type of function or the way that you're going to interpolate between the data. So, it's less of a, you know, how well can I predict my training data but, more the more parameters you have, the more efficiently, or more effectively, you can interpolate to get those smooth curves between the data points. "
            },
            {
                "timestamp": "59:00",
                "text": "And, so that interpolation limit about going from this blue curve down to this black curve, that's where having more parameters helps you. Okay. Um, oh, yeah, one last one at the back. "
            },
            {
                "timestamp": "59:30",
                "text": "I just wanted to ask, um, for the similar, like, the similar data analysis shown on this plot, like, simple, uh, function approximation smooth, uh, Compared to, like, traditional methods like smoothing splines. Yeah. Uh, what advantages does machine learning have, or kind of this sort of, uh, architecture? Okay. So, great question, so, I would say fundamentally it's very similar. There actually some interesting connections between sort of spline-based methods, especially smoothing splines and like this type of thing. Um, more practically, though, splines tend to struggle a lot in high dimensions, right? You sort of get this curse of dimensionality effect, you can't sort of have enough knots to sort of characterize the whole space. So, you can think of what neural networks and these other functions is doing is being much more efficient ways of interpolating in high dimensions. "
            }
        ]
    },
    {
        "video_id": "7LH01TSqou8",
        "transcript": [
            {
                "timestamp": "00:00",
                "text": "The visitor today is Dr David Hendricks who is a uh recent PhD graduate from the University of Surrey uh David uh is an astrophysicist specializing in black hole formation primarily stars and population synthesis studies."
            },
            {
                "timestamp": "00:30",
                "text": "He earned his bachelors and masters degrees uh undergraduate in astrophysics from the University of Amsterdam and recently completed his PhD at the University of Surrey the masters student uh been Cosmos group in Amsterdam he started working on population studies of populations synthesis since his PhD was about primordial populations mass transfer uh and compact object formation currently David is analyzing data science projects using techniques like gravitational wave detectors and space interferometers he is actively seeking postdoctoral positions aiming to continue contributing to astrophysics."
            },
            {
                "timestamp": "01:00",
                "text": "So uh thank you and great thank you for this uh introduction um I'm going to talk about pulsational pair instability supernovae and gravitational wave and electromagnetic transients uh I see some familiar faces here and you have seen in part of this presentation already I've added some things I've taken some things out um But so uh you you may be familiar part with this story"
            },
            {
                "timestamp": "01:30",
                "text": "uh it's been a couple of good years now that uh there has been gravitational wave observations um since 2015 there has been gravitational wave observations specifically black hole binaries mostly uh that have been observed and as you see in in this graph on the left especially in O3 things have really started to ramp up uh and we are definitely getting enough observations for statistics of these observations It's not only one or two observations It's getting distribution level."
            },
            {
                "timestamp": "02:00",
                "text": "um especially O4 is currently ongoing that will change this graph up to another order of magnitude higher in detection rate and uh uh so we're we're really at the at the forefront of like a new era of like gravitational wave observations um That these observations allow us then to make distributions of the of the properties of these of these mergers."
            },
            {
                "timestamp": "02:30",
                "text": "And as I said it's mostly binary black hole mergers that we find and what we can do is we can look at for example the distribution of the primary masses of these systems Primary mass in this case or with it means that it's like the most massive black hole of the of the system and there are some interesting features that we can see here uh some of them we expect to see and some of them we expect to see in a different place and specifically"
            },
            {
                "timestamp": "03:00",
                "text": "uh the the peak that we see in this region here um is interesting because it's found in a location that is not entirely expected It's a lower mass than we expected from um well from th from physics and stellar evolutions we know and my my quest here is I'm going to try to reproduce the bump that we find in this distribution and I'm going to try to if I cannot reproduce it how can I change the physical assumptions so that it does match this bump."
            },
            {
                "timestamp": "03:30",
                "text": "And what does that then imply about stellar evolution and other astrophysical properties down the line um I'm so I'm talking about gravitational wave mergers There's a whole bunch of different types and these uh they originate in different environments and they can probe different properties of the source and they are affected by a whole bunch of physical processes And I specifically I'm going to look at the contours for at the inspiral wave signal."
            },
            {
                "timestamp": "04:00",
                "text": "But as you can see here there's a whole large spectrum of gravitational waves and we're really just at the at the start of an era of gravitational wave observations uh relatively easiest one to observe is the inspiral phase and that's what I'm going to talk about today uh and that is um write on the interface here terrestrial interferometers space interferometers maybe you've heard that the LISA project has been given green light uh which will be launched in a decade or so and that will be a space interferometer."
            },
            {
                "timestamp": "04:30",
                "text": "um that will give us access to a whole new uh sort of parameter space of mergers new mass range basically um Right so I'm going to talk about merging compact objects uh not about cosmological vibrations or supernova explosions that generate gravitational waves but these are merging compact objects so binary black holes in my case but it could also have been neutron star mergers or binary neutron star mergers."
            },
            {
                "timestamp": "05:00",
                "text": "And predominantly they occur in different in a couple of different regions um relatively the easiest to handle is called the field and it's basically anywhere which is not in a particularly over dense environment or uh environment with extra gaseous uh material like AGN disks and that's that then are the two other uh main main locations where these systems can be born and these clusters they these other two scenarios they uh impose extra effects or extra uh torques on the binary system or extra interactions And that regard the field is easy cuz the binary can be considered"
            },
            {
                "timestamp": "05:30",
                "text": "uh isolated And so we're talking about binary systems but s wanting about stellar evolution stellar evolution is quite complex People have been working on this very long but it's more or less It's It's somewhat understood right We We know certain properties affect the evolution more than others uh and in particular the mass is a very important quantity that changes the evolution of a star uh the more massive the hotter it is faster it will evolve and it will change whether it will go supernova or not."
            },
            {
                "timestamp": "06:00",
                "text": "And then there is the composition which is the secondary but also very important effect is that stars with initially more metals in them so higher metallicity will basically lose more mass over their lifetime And so if you have a black hole of a certain mass um there is a region of metallicities that it cannot have been born with because"
            },
            {
                "timestamp": "06:30",
                "text": "ppppp"
            },
            {
                "timestamp": "07:00",
                "text": "There's basically no way of to start to retain as much mass as it needs to form a black hole like this. So these two are the are the the most important players that we should care about now for single stars but binary systems they introduce a whole extra set of things right?"
            },
            {
                "timestamp": "07:30",
                "text": "Now we know that a binary system is two stars that are gravitationally bound to each other um they form in associations but if they're generally formed in the field they'll live relatively isolated lives um and they their properties are distributed by distributions that we start to understand to a bit or at least observe and we can work with this."
            },
            {
                "timestamp": "08:00",
                "text": "Things like the IMF for single stars we use it for binary stars as well but then there's period and mass ratio distributions that basically inform us about what the distribution of these properties of the binary systems is."
            },
            {
                "timestamp": "08:30",
                "text": "And we can observe or infer them and then we can use this as initial distributions for our binary systems um the difference between binary systems single stars other than well because they come in pairs they can start to influence each other and it's predominantly the influence of the transfer of material between the two stars that really changes the game."
            },
            {
                "timestamp": "09:00",
                "text": "Uh there's tidal interactions like uh like the Earth and the Moon have um and tidal interactions are very important but if mass is being transferred back and forth uh that can really affect the system very strongly um so this is something that we have to take care of or take into account this uh"
            },
            {
                "timestamp": "09:30",
                "text": "but importantly for me binary stars become binary black holes and this is the uh that's why I'm looking at these binary stars then one point I want to mention about Roche lobe overflow or the transfer of material I'm not sure if this is covered in in undergrad courses here or if people work on this but very briefly if a star uh during its life star evolves and it changes its size. Now single stars are in that sense not hindered by any any thing in they can grow sort of indefinitely large."
            },
            {
                "timestamp": "10:00",
                "text": "Uh but if if uh uh uh material becomes unbound at least from M1 and can flow between the two stars via a a accretion stream which ideally in an idealized situation looks like this and then it either directly impacts the star or forms an accretion disk."
            },
            {
                "timestamp": "10:30",
                "text": "This interaction there is a lot of stuff going on here and that it's it's the key player to determining the the outcome of binary stars. Specifically is that this this transfer of mass can be is become look but it can be stable or it can be unstable and in unstable mass transfer basically means that any extra bit of mass that is transferred leads to more mass being transferred and it's a runaway process that just ends in uh either just two stars merging or uh or common envelope that shrinks the system significantly and that's something that I will show you in my results that these two different scenarios they they probe a different regime of mass."
            },
            {
                "timestamp": "11:00",
                "text": "Uh but about population synthesis is that I've got all these initial distributions of binary systems all these interactions that we can care about but now I want to model binary systems and calculate how many black holes we see in which different properties there are."
            },
            {
                "timestamp": "11:30",
                "text": "Um and I can do this with a with a technique called population synthesis and it basically means that I want to synthetically model a whole bunch of stars. Uh the synthesis part means that I'm not solving the full stellar equations but I'm taking so shortcuts I either using interpolation tables or analytic fits to stellar evolution but that allows me to evolve stars very quickly. So if you want to do a full detailed stellar evolution that takes quite a while but if I do this in a a synthetic way then it speeds things up by a million a million and then I can do a lot of stars very quickly."
            },
            {
                "timestamp": "12:00",
                "text": "Uh because if we want to estimate the rates of something that's quite rare then we need to evolve a lot of stars to probe this. Uh this so you know this event. Um right and so also what's interesting is that sort of a purpose but not really. It's I've drawn it as a black box that it gets some ingredients and some some configurations and then I put some in initial distribution in there and I get something back."
            },
            {
                "timestamp": "12:30",
                "text": "I mean we do know what's going into this evolutionary engine but there's so many processes that work in tandem with each other that it's still sometimes quite difficult to to estimate what's going to happen to a system of an initial configuration. It's not always very clear so sometimes you just have to see what comes out and then start to understand the relation between what in and what goes in and what comes out."
            },
            {
                "timestamp": "13:00",
                "text": "Uh I want to do this for a large number of stars as you said large number of metallicities which is the the other property that I mentioned and then I can start varying certain properties."
            },
            {
                "timestamp": "13:30",
                "text": "Um and specifically I want to vary the supernova mechanism because what I'm actually looking at is very massive black holes uh of the order of 40 solar masses which are basically only produced by uh supernova uh of a certain type."
            },
            {
                "timestamp": "13:54",
                "text": "And especially this feature that we find is always theorized to come from a specific kind of supernova which is called No right sorry I evolve all these populations of stars and then I combine that with the star formation rates history and now I think quite some people here work in galaxies or or like large scale structured things. So you are aware of like how things work with cosmological star formation rates and how they uh influence for example AGN formation and stuff."
            },
            {
                "timestamp": "14:00",
                "text": "supernova types that I really care about is it's called pair instability or pulsation or pair instability supernovae"
            },
            {
                "timestamp": "14:00",
                "text": "I use a cosmological star formation rate because currently I don't really care about what source what galaxy source it comes from but I'm working with Rob Yates to to zoom in on like the galaxy sources of these of these binary black hole systems."
            },
            {
                "timestamp": "14:30",
                "text": "And it basically occurs for stars that are very massive uh about 35 solar masses after having undergoing like after the main sequence basically they have a helium core which is quite evolved and that has to be 35 solar masses or higher"
            },
            {
                "timestamp": "15:00",
                "text": "Um and that means that initially they are quite a bit more massive than this Um what happens in these stars is that the the the density and temperature are such that photons can interact with the with the with the surrounding plasma and create electron positron pairs"
            },
            {
                "timestamp": "15:30",
                "text": "Even briefly but that removes the radiation pressure from the system And these stars at this stage in their life they are mainly held up by radiation pressure which is already a slightly on like quasi-stable state of a star And if you then start to remove this extra pressure that's necessary for the stability then that can sometimes lead to uh the instability a runaway uh that makes the star collapse onto itself"
            },
            {
                "timestamp": "16:00",
                "text": "And that's basically going from two to three This collapses onto itself heats things up so so much that it can ignite um certain fuel And if there is enough oxygen in this case then that can lead to an explosion that's quite significant And this explosion can lead to uh three outcomes"
            },
            {
                "timestamp": "16:30",
                "text": "Um I'm going to Yeah So first 4A the explosion is so strong that it disrupts the entire star And then you get a huge you know supernova remnant cloud but no remnant object no compact object in the center So there's no black hole at this point"
            },
            {
                "timestamp": "17:00",
                "text": "or this explosion is not strong enough to remove all of the material but only some of the material and then you get this this effect of okay some mass is removed but not all of it Or the explosion is not strong enough to remove any mass and the star keeps collapsing onto itself"
            },
            {
                "timestamp": "17:30",
                "text": "This is only possible for the most massive stars and at this point I'm currently not considering this to be relevant to to my conquest like my quest here to to the to finding out where the peak comes from right I'm mostly concerned about these stars that that pulse that release behind the remnant And this pulsing behavior sort of sort of caps the maximum mass that the star that the black maximum black mass that the black hole can leave behind"
            },
            {
                "timestamp": "18:00",
                "text": "Uh this process can occur a couple of times Uh the the the center is very hot the it's cooling via neutrinos It's very efficient It starts to relax onto itself again and then it could undergo the same process two two times maybe three times and then if it's still not uh Well then it will explode in the normal normal sense or core collapse supernova"
            },
            {
                "timestamp": "18:30",
                "text": "But so this pulsational mechanism is something that can limit the maximum mass of the black holes And if you put this into mesa or any detailed stellar evolution code you will see that stars of a certain initial core mass for helium core mass or CO core mass that's at this point uh Well we take that as being the same thing currently"
            },
            {
                "timestamp": "19:00",
                "text": "If you put that into mesa and you evolve these stars then you see basically three regions forming This blue region is the quasi sort of conventional core collapse There's no instability occurring in the star It just evolves up until the the the iron core cannot provide any fusion anymore and star collapses onto itself"
            },
            {
                "timestamp": "19:30",
                "text": "Explodes or not But then you get the core collapse In this region the star partially becomes unstable because of the reasons that I just explained But the explosion that follows doesn't explode the entire star But what you do see is that if you draw a line from here to here you see that material gets removed and the the the remaining remnant is slightly less massive as the pre supernova star"
            },
            {
                "timestamp": "20:00",
                "text": "And you have the final regions pair instability region where there's basically no remnant anymore And we can capture this in a simple fit So we can just say okay for uh uh uh CO core mass that we put in there you build a fit that predicts what the outcome black hole mass is And depending on how you make that fit uh what you use as the as the actual parameter you get two different fits"
            },
            {
                "timestamp": "20:30",
                "text": "Uh one that matches correctly with the previous um non exploding part and one that doesn't Um my fiducial fit is basically uh we fixed the previous fit and now it's just a continuous line uh and from this circle point onwards this exploding mechanism works And they you see from here it starts deviating and there's this this region of material being lost basically"
            },
            {
                "timestamp": "20:59",
                "text": "But what we're predicting here is a relation between the initial core mass and the final black hole mass and what we see here is that from the maximum black hole mass that we can find is about 50 solar masses"
            },
            {
                "timestamp": "21:00",
                "text": "And so we basically shouldn't expect any any black holes above this point right Um because black holes that form at this point form at 20 solar masses more or less they originate from more massive stars but they lose so much mass that they end up as quite a bit lower than uh whatever is left of there Now if I put all of this into my machinery I calculate all of the binaries binary stars a couple of million of them different metallicities combine this all with the star formation rates I get a distribution which is more or less similar to what we observe"
            },
            {
                "timestamp": "21:00",
                "text": "And then on the orange we see my distributions what I predict"
            },
            {
                "timestamp": "21:30",
                "text": "On itself not entirely bad but there's a couple of things that we see here Um we see a peak at 10 solar mass which more or less matches but not entirely which is already interesting and we see another peak not as convincing and also not as dramatic at a location which is not the observed location"
            },
            {
                "timestamp": "22:00",
                "text": "And so at this point there is something that's It's not matching up right the second panel basically shows that whatever uh black hole we find what's the fraction of those black holes that actually formed through this mechanism that I've talked about And we see in this region all of the black holes that form in here are formed through this pulsational pair instability mechanism so indeed this this peak here is is caused by pulsational pair instability"
            },
            {
                "timestamp": "22:30",
                "text": "And what the problem was everybody always thought that this peak at 35 solar mass was caused by pair instability supernova and that's not what we're finding And now there is a bit of a of an issue uh or our input models wrong uh or do we need to change something or what Or is this peak actually not caused by pair instability supernova That If it wouldn't be there's quite some implications down the line that I could go into maybe at the questions"
            },
            {
                "timestamp": "23:00",
                "text": "But that would have quite some some conclusions basically if it would not be caused by this Um some extra things that are interesting here is that in in this panel we see the eccentricity of the black holes upon their formation And we see that from more or less 30 solar masses black holes that do merge in the lifetime of the universe and that we can observe now predominantly uh form very eccentric"
            },
            {
                "timestamp": "23:30",
                "text": "And this this high eccentricity basically leads to their merger time becoming a lot shorter than it normally would be so uh a circular binary takes longer to merge than an eccentric binary because there is this this this eccentricity where they come close to each other which loses a lot of angular momentum through gravitational waves Uh so there is something going on that makes these systems merge Conversely"
            },
            {
                "timestamp": "24:00",
                "text": "that means that there is some physics that we can test in this regime Now uh because whatever causes this high eccentricity at least in field stars is caused by the by the supernova kick of the companion Because the black holes that form the primary mass black holes form so massive that they don't really kick anything so it's the companion that kicks uh the system With kick I mean there's an asymmetric uh removal of material from the star that leads to a momentum being imparted on that star needs to an increasing eccentricity"
            },
            {
                "timestamp": "24:30",
                "text": "And that basically means that we can start using this area maybe as a way of testing what supernova kick models might work or not Um And then lastly this panel here is the Roche lobe overflow type which I alluded to earlier some sits Some mass transfer is stable and some mass transfer is unstable"
            },
            {
                "timestamp": "25:00",
                "text": "And this um is unstable mass transfer there is some some uncertainties in this because there is a lot of uncertainty in the um the requirement of this instability and the models that we've been using are probably a little bit out of date especially for these very massive stars and so we thought well maybe if we just remove those systems that originate from the common envelope or unstable mass transfer um That leads to at least the peak being matched matched a little bit better"
            },
            {
                "timestamp": "25:30",
                "text": "And now this isn't entirely fair by just removing these systems right but what it does show is that the physics that goes into this stability criteria and the stability calculations may well have a big impact on whether this part here fits And so whereas this high mass part can be used to constrain something about the eccentricity of the supernova kicks This lower part may be used for the mass transfer properties of these systems"
            },
            {
                "timestamp": "26:00",
                "text": "They those are also extra uh nice things that I found but the main thing that I found is that whatever I put in here doesn't lead to a correct location for the peak It's it's about 20 solar masses to the right uh of what we needed to be right Now is there something we can do about this Is there some uncertainty that goes into these models that we can s- start to wiggle around with uh maybe maybe some physical assumptions that are quite uncertain uh that we can invoke to say well let's introduce some extra changes in this and let's start putting things left or right Well yes um but most of the things that go into this at least in the study that Rob Farmer did uh seem to show that things were quite stable against these uncertainties these uncertainties did not lead to a large range in a a black hole masses so"
            },
            {
                "timestamp": "26:30",
                "text": "What we see here is some numerical assumptions physical assumptions and environments that lead to uh a range of black hole masses within the the results from varying those those parameters basically The only big one that leads to a very large shift in the in the maximum black hole mass is the the rates and the rates that means the the nuclear reaction rates of the relevant uh reactions that the carbon to oxygen uh reaction rate"
            },
            {
                "timestamp": "27:00",
                "text": "But um I was hoping for a little bit more wiggle room And uh I started looking a little bit further And indeed the reaction rates are quite important and especially if you if you use better resolution then things start to shift a little bit even more more so than we we found before But also things like the rotation additional cooling and some uh convection and um Well there is there is a bit more wiggle room than we than we may have thought before"
            },
            {
                "timestamp": "27:30",
                "text": "And these these things that I'm listing here basically affect two different things Either it affects the CO core masses that undergo these explosions As I said earlier from a certain range things start to explode or not So this first block will shift that to the left or the right And this second block it basically induces additional mass loss so even uh the the range doesn't really change but the explosion"
            },
            {
                "timestamp": "27:59",
                "text": "pppppp"
            },
            {
                "timestamp": "28:00",
                "text": "leads to more or less mass loss And there's a couple the here that I've uh with an asterisk too."
            },
            {
                "timestamp": "28:00",
                "text": "ppppppp"
            },
            {
                "timestamp": "28:30",
                "text": "Uh these asterisks indicate beyond standard model physics Um so it's it's speculative but that might actually lead to a testable way of um excluding this beyond standard model regime that they that they use for example the axion masses that they used for uh for this axion parent stability We could maybe use what we're finding here to exclude this or confirm what they're suggesting Um that's a big stretch though but uh so there's a couple of extra things that we can throw in the mix And one of these is indeed the axion instability which is a very similar stellar instability but it's not caused by the formation of electron positron pairs."
            },
            {
                "timestamp": "29:00",
                "text": "What it's now caused by the formation or the interaction with a certain axion And this axion depending on the mass of this axion it will change when this instability occurs And so in these different different regions you see here on the left plot different colors indicate axions of different masses And they will lead to instabilities for stars of different tracks basically And specifically if you would look at the the the right plot if the the blue the blue lines are the standard model SM um if you would then introduce an axion with the same mass as the electron if that would exist And this this this mass range is not if I recall been excluded entirely uh by by other observations."
            },
            {
                "timestamp": "29:30",
                "text": "If you if that would exist then that would lead to a shift in the in the core masses that actually explode all the way to the left with more or less the same number as that I need to to match the peak And that's why I was like okay this is quite interesting and it also might actually lead to something that works for me Let's consider this a little bit more or in that in that case let's use this as a potential motivation of shifting things a little bit further uh out of the normal physics regime right Anyway um axions with higher masses don't lead to a larger shift in the location because the instability would occur already due to electrons right But so only if it's like electron mass or slightly lower will it actually change this this region of stars that explode to a lower lower mass um In a slightly less speculative fashion we can also just take a stellar model and do it in a more detailed way um There's been no improvement in this in the reaction rates of this carbon to oxygen."
            },
            {
                "timestamp": "30:00",
                "text": "No improvements depending on who you ask It's a very debated uh reaction rate But if you put that into a detailed stellar evolution code like MESA and you properly deal with the numerics then that leads to a shift in the location of these black holes as well Um in this case it goes instead of leftward to to where I needed to go it goes upwards to the right Uh and so these updated models these better supposedly better uh better treated and and more up-to-date rates lead to a shift in the wrong direction for me So now I wasn't entirely happy with that but uh it is what it is So the Farmer models were the ones that I was using before And you see that they lead to a maximum mass of more or less 50 solar masses."
            },
            {
                "timestamp": "31:00",
                "text": "But now with some updated rates and specifically updated uh configuration for the for the numerics of of of these of these models you see that these these lines extend to a higher mass higher maximum mass Um more or less 60 even And in some of their um more extreme versions that would go up to 80 or 90 even so there is a lot of wiggle room um But but this motivated me or sort of uh allows me to introduce some free parameters in my model that I could shift left or right quite a bit uh invoking okay either axion pair stability or or these other models to shift them around a little bit What I what I introduced here was extra parameters that basically uh lead to the same effect This curve shifting to the left or the right And and inversely then also shifting to lower maximum masses So if you shift the orange line to the left the maximum mass will also be a lower location And for the blue line if you shift it to the right so a higher mass will start to become unstable then it also leads to a higher total higher maximum black hole mass of So you can shift things left or right The question now is um how much are we allowed slash uh well motivated enough to shift And can we find anything that fits um And that regard yes I can find things that fit um So anything that is so my orange is uh is still the fiducial model which uh which I showed earlier But now I can uh introduce shifts to to the left of stars that are lower mass that start to become unstable or stars that are higher mass that start to become unstable."
            },
            {
                "timestamp": "32:00",
                "text": "And what I'm finding is that more or less 15 solar masses of reduction of the mass that undergoes this instability is necessary to match this peak which is then this green line here That's more or less the correct location Maybe 14 maybe 13 And this was explained by something like this axion parent stability shifting things significantly downwards right Or rates being updated in the in the different location But now with this um with this updated rate that they showed just now things would actually move upwards with more or less 10 solar masses And so now I get these two peaks um at minus 15 And uh at plus 10 that I'm going to consider as the two extremes of my of my next step because"
            },
            {
                "timestamp": "33:00",
                "text": "I now have a fit or something that fits the observations barring some some other things But that's not entirely fair right I mean to do"
            },
            {
                "timestamp": "33:30",
                "text": "pp pp pp pp pp"
            },
            {
                "timestamp": "34:00",
                "text": "pp pp"
            },
            {
                "timestamp": "34:30",
                "text": "pp pp"
            },
            {
                "timestamp": "34:50",
                "text": "pp pp"
            },
            {
                "timestamp": "35:00",
                "text": "using free parameters in my model and just start shifting things and you know at some point I can make a fit."
            },
            {
                "timestamp": "35:00",
                "text": "pp"
            },
            {
                "timestamp": "35:30",
                "text": "So I do need some secondly observable to start to constrain things right."
            },
            {
                "timestamp": "35:59",
                "text": "When you say fit the model you're not you don't say you've got a tail there is Yeah yeah yeah right Yeah and so this tail is indeed important."
            },
            {
                "timestamp": "36:30",
                "text": "And so this tail and that's a this this set of where the speak."
            },
            {
                "timestamp": "36:59",
                "text": "Well it speaks basically sets a maximum limit that single stars can attain right or binary stars and that matter but stars in a cluster can merge continuously or hierarchically."
            },
            {
                "timestamp": "37:29",
                "text": "So once they have merged and they have a system with one mass here they can continue to merge to populate whatever is up here."
            },
            {
                "timestamp": "37:58",
                "text": "And so they they have used the location of this peak to sort of constrain the contribution of global cluster originating black holes."
            },
            {
                "timestamp": "38:29",
                "text": "There are other mechanisms to populate this this region but they're all either probe not of high enough rates or kind of secure or debate that they actually populate this region and the largest contender to actually populate this larger region here is cluster evolution or continuous merging within clusters."
            },
            {
                "timestamp": "39:00",
                "text": "And so indeed I cannot model this because I only treat stars as being from field binaries that are not in an environment where they can continuously merge up until higher masses."
            },
            {
                "timestamp": "39:30",
                "text": "So yeah I'm missing a tail but maybe that's um that's sort of like expected within my approach already."
            },
            {
                "timestamp": "39:59",
                "text": "So now I do fit it and now what I I do need some extra observable because otherwise it's not really fair right Um."
            },
            {
                "timestamp": "40:29",
                "text": "And that's that's why I want to use transient rates because we're talking about supernovae these things explode and they are quite luminous and that's something that we can observe There are many uh surveys that observe the rates of supernovae."
            },
            {
                "timestamp": "40:58",
                "text": "Maybe we can use this to constrain something about uh our our liberal uh changes to the model basically So what I'm currently showing here is a a build-up of of different rates um first of all the earliest observations of binary black holes and what I'm showing here is the event rate density of these of these events."
            },
            {
                "timestamp": "41:28",
                "text": "So this is the rate of these events occurring per gigaparsec cubed so within a certain volume of of space Um and."
            },
            {
                "timestamp": "41:58",
                "text": "all of these detectors can sort of uh calculate this sort of um they don't spend the same the volumes of course but they can calculate things in in in that volume."
            },
            {
                "timestamp": "42:00",
                "text": "uh more less 6-10 minus 6 times that of the core-collapse supernova rates but is based on on this on this super luminous supernova And that basically means that every movement I make up only creates more tension with the super luminous supernova And so if I move if I change this mechanism of parent stability to match this peak in the gravitational wave distribution there is an increasing tension with the super luminous supernova rate that I'm creating And basically this cannot be the solution to to this peak which is even strengthened by the fact that if you go upwards you make better match with the the the the detailed models and it releases the tension And so basically from this I need to I need to conclude that at least in my approach to calculating parent stability supernova it's excluded by the super luminous supernova rates and I cannot use it to invoke uh it as a solution to this peak"
            },
            {
                "timestamp": "42:29",
                "text": "So binary black hole merger rate which is the thing that I've been calculating now and we have the super luminous supernova rates which will be a a key part in this next next part I'm sorry and the core collapse supernova rate."
            },
            {
                "timestamp": "42:30",
                "text": "And now there are others proposed solutions to this peak And the And this sort of the uh the implications that this peak is not from parent stability is also quite uh well this has to do with this clusters If this peak lies somewhere else and the contribution of clusters is quite different um But there are other ways to find something like a peak uh And And Max Briel showed that if there is a nice good combination between stable mass transfer even those super-Eddington uh mass transfer and chemically homogeneous stars the combination of these three uh And that together with winds leads to a peak in the location that actually is observed to have this peak um And so only a very small part is uh is contributed by this parent stability supernova because um the models that he used still have this range embedded in it So there seems to be some other pathway to getting this peak"
            },
            {
                "timestamp": "42:30",
                "text": "um there are other dynamical models that could lead to an over density within this region like The only thing with this paper is like they still have this parent stability which leads to this 40 solar mass uh even if you remove it you still see this you don't see this peak anymore So parent stability is some part of this of this story But they do see somewhat of a peak without uh such contributed by dynamical effects as well there And"
            },
            {
                "timestamp": "42:30",
                "text": "then another paper which came out relatively recently in December um which shows that there are some secondary effects of this parent stability supernova that have my maybe been overlooked So the model that I used was slightly a toy model I took a fit and I introduced some sliding parameters in this And the way that they have approached it in this way they have They have basically made this sliding calculation within the detailed uh code and they found that there might be some secondary effects in here The only thing I have with this paper is that what they find is that if you slide things to the left So this is the their fiducial model where you see the black hole mass and the the rate of these black holes merging If you slide things to the left that would still increase the tension that I'm finding with this supernova rates So even if they find the secondary peak which is slightly lower which they say that okay that could match that could be the match at 35 solar masses There basically moving leftward would still make a higher tension with this super luminous supernova rates"
            },
            {
                "timestamp": "42:30",
                "text": "um to a second like a a tertiary path that we could maybe use is to calculate the yields of these pulsational parent stability supernova and put this into a galaxy evolution code and this is something that I'm working on with with uh Rob here um we do need uh some better yield sets for these supernova But we've been working on uh an interfacing between his code LG Galaxies and our code called Binary SEE um So that we can interface these these yields of these supernova quite easily and we can propagate that through his calculations um we're not entirely there yet because we need to now run some detailed models But um quite quickly we could also use maybe the path of galactic chemical evolution to exclude this in in a in a third way basically This is one of the things that we that I find in a very rough back of the envelope calculation is that um at this point of shifting things to the left to match the peak more than more than uh 1% of all the mod material that's being generated comes from this sup these parent stability supernova and that might well be intention with some of the yields that we uh we observed But it's it's still a little bit too early to say whether that's actually creating tension or not And we probably need a a better code than a back of the envelope calculation like I'm doing there to actually estimate this um But yeah there is different pathways on which we can exclude uh the fact that pulsational parent stability is the cause of this and and just to wrap it up I've implemented this new parent stability mechanism Clearly however my fiducial fair version of this does not lead to a good match I need to introduce some changes uh And I can make a match but that match is tension with uh observed transient rates And um Yeah that basically makes me conclude that this bump is not caused by parent stability and that will have implications down the line on things like how the big the fraction of contributions of globular clusters to to all of the the merger rates And that means something about the formation of globular clusters or this Yeah Right Okay That's what I wanted to uh to show today Thank you David for that very interesting talk um So we've got a few minutes for questions Yeah I always sort of uh thinking about this this morning about the non-matching with the kind of great distribution that you have in there because of this p pp p p"
            },
            {
                "timestamp": "42:59",
                "text": "And what I'm going to add to this is my models basically Um I'm showing this binary black hole merger rates which is these black dots which all most matches with the observations but we shouldn't forget that I removed this common envelope events."
            },
            {
                "timestamp": "49:00",
                "text": "Recap Concluding: We implement a new prescription for PPSN mass loss Our fiducial version does not match with observations Motivated modifications lead to a match But they are in tension with other observables More likely that the bump is not caused by PPSN* This has strong implications for other studies!"
            },
            {
                "timestamp": "49:30",
                "text": "Other explanations of peak: cluster dynamics Semi-analytical cluster dynamics code for binary black holes Combination of: Dynamical effects Pairing of BHs"
            },
            {
                "timestamp": "50:00",
                "text": "Transient rates Rate of CCNS at z=0 matches What about (P)PISNe?"
            },
            {
                "timestamp": "50:30",
                "text": "Yeah, the the great thing about that, I'm wondering if we can say a little bit more about the the origin of that that great curve because it doesn't look like too many other observational distributions I've seen."
            },
            {
                "timestamp": "51:00",
                "text": "I mean, and you know that I mean, in the sense that it's very smooth and doesn't Oh actually it will misbehave or Well yeah, no right. With it like Like Yeah yeah. So underlying in this is just a handful of observations, right?"
            },
            {
                "timestamp": "51:30",
                "text": "Yeah. Uh so there is a reason there is a reason why this is such a smooth curve. Yeah, it's basically because they they fitted a power law plus peak model to to this curve to this set of observations. And within the uncertainties of the observations that leads to the the sort of like the the regions spanned by this by this curve. There are other fits that people have made and even non-parametric fits uh but they all do find a feature in that same region."
            },
            {
                "timestamp": "52:00",
                "text": "So, this is not necessarily just a remnant of the fact that people impose a power law plus a Gaussian peak because then you would find something like a Gaussian peaks somewhere, right? But, this is actually of all the all also, all the other models."
            },
            {
                "timestamp": "52:30",
                "text": "the simplest model that actually captures the feature that all of the models see. And otherwise it becomes an over an overfit model that you see, so many to ring things. But, it's also kind of thinking of you know, if they'd had they'd have been looking at simulations to the past and same things like you know the the observations that simulations actually really And when you're talking about some of the something this Yeah, yeah yeah. But, so between"
            },
            {
                "timestamp": "53:00",
                "text": "like I go all the way back to the first like between the first dataset that was uh released, basically all the O2 and know O1 this curve this location of this peak was also found right? So, now with the increase in number of of of with an increase in numbers uh things got constrained but it it remains a feature in this location. And, even if there is a relatively simple fit made to it uh even with more complicated fits or non-parametric fits."
            },
            {
                "timestamp": "53:30",
                "text": "a feature is seen in that region right? And it's relatively easy to uh to show that has the distribution with the side mark that this is actually behind there."
            },
            {
                "timestamp": "54:00",
                "text": "The questions I could also check the underlying productions just to make sure we see it through this. No questions there? Thanks for asking. So, uh I've got a question in this uh stellar evolution model where you have these big mass loss events like you described in this several of them but it seems that the specifics of this and it must be very deterministic uh in order to give us the sharp cut off on the mass."
            },
            {
                "timestamp": "54:30",
                "text": "Right. is is that the case and to what extent in the models do you have to take into account the details of uh these like of this stellar evolution of these events or is it less sensitive to that. So"
            },
            {
                "timestamp": "55:00",
                "text": "um right in in this figure we see uh Right, so we see a whole bunch of different curves and these curves originate from stars with different metallicities. And so there is some some change in the in the opacity that basically is the result of these different metallicities that leads to a different structure and a different explosion mechanism or behavior. But otherwise if you treat to start the same and uh stellar evolution leads up to a core mass of that mass then you you should expect more or less the same uh the same outcome. However, we are treating these things still in in 1-D stellar evolution codes and in some supernova mechanisms it's also quite clear that the initial turbulence of convection has quite a big effect on the outcome of of these explosion right?"
            },
            {
                "timestamp": "55:30",
                "text": "Um so, I think these results are fairly robust but a good set of 3-D 3-D models with the same global structures but different microstructures might be interesting to to lead to Yeah. Uh yeah. But I do think globally this is quite a robust result. And, I find simulations are like information of star masters so, we don't have so much information."
            },
            {
                "timestamp": "56:00",
                "text": "And, we find simulations that are like information of star masters so, we don't have so much information of any of the research that this probably Like, get the stars to misbehave Um I know of a couple of papers that that seem to have a result in that direction but I don't know entirely for sure, I could look up the papers but these are"
            },
            {
                "timestamp": "56:00",
                "text": "Right so the O4 uh"
            },
            {
                "timestamp": "56:03",
                "text": "uh data will well the O4 run is currently ongoing."
            },
            {
                "timestamp": "56:09",
                "text": "Um so that they will definitely release a whole"
            },
            {
                "timestamp": "56:15",
                "text": "a whole bunch of extra material that we can start"
            },
            {
                "timestamp": "56:18",
                "text": "to use and then this curve will"
            },
            {
                "timestamp": "56:21",
                "text": "really hopefully start to show some more features that we can use as anchor points right because what I I do predict that if in my better model the one that's least intention with the with the uh transients there's an another peak here it's all basically waiting on"
            },
            {
                "timestamp": "56:36",
                "text": "on those results and otherwise there's transient transient factories like the BTF and ZTF that we can use for this"
            },
            {
                "timestamp": "56:44",
                "text": "um but I really think for me the new uh gravitational wave run is is going to be um the the one that can"
            },
            {
                "timestamp": "56:54",
                "text": "exclude or bring even more attention to to to the situation right But but currently the this this transient tension is already there and it won't relieve it So what I'm predicting is that there is another feature in this region here and that would be basically a sort of confirmation to to what I'm predicting"
            }
        ]
    },
    {
        "video_id": "2BXLR-UYUns",
        "transcript": [
            {
                "timestamp": "00:00",
                "text": "All right. Well, let's uh let's see it. So, we're going to continue with the first uh session of contributed talks. We have four exciting talks this morning, and uh Rafael Martinez here at AstroAI, and uh we're going to talk mostly about stellar astrophysics in this session. Uh the first speaker is Nolan Kobliscke, from also from Toronto, as you're going to be telling us about modeling spectra and tuning into stellar foundation models. The floor is yours."
            },
            {
                "timestamp": "00:30",
                "text": "Thank you. Hi everyone. So yeah, I'm a first-year PhD student at the University of Toronto, and today I'm going to be tuning you guys into SpectraFM. Uh uh and and and and  uh So yeah, I'm going to go about how we're making a foundation model for stars. Uh jumping right in."
            },
            {
                "timestamp": "01:00",
                "text": "Okay. Uh so yeah, so the dataset that we're using to just as a prototype, prototype our foundation model is the APOGEE spectroscopic survey. It's looked at hundreds of thousands of stars and got high resolution spectra for each of them as well as, you know, some simple spectra property or stellar properties like temperature and also chemical abundances. And this has been used for many, many years to train machine learning algorithms because of the size of the dataset. Here's just a few examples, uh you know, a neural network trained on hundreds of thousands of APOGEE spectra to predict distances, scientifically useful, right? Another neural network trained on tens of thousands of APOGEE spectra to predict ages from astro cosmology. Great, also scientifically useful. Um but what if you don't have tens or hundreds of thousands of stars to train a model for your task, but you want to use uh a neural network to do so?"
            },
            {
                "timestamp": "02:00",
                "text": "So, uh what if you only have 100 stars? This is very common. You launch a new instrument, you collect, you know, all your spectra, but you want to train a neural network and you only have 100 labels. You only have 100 iron abundances measured in this scenario. This is probably not going to cover the full distribution that you want your neural network to be good at. In this case, if they're all metal-rich stars, then training a neural network is going to lead to a model that's useless in the metal-poor range. Um but there's some clever tricks to get around this. You need to find more data. Uh so you can leverage like synthetic spectra, spectra from different instruments, even uh different but similar tasks. Uh you can train up a model that has a good general knowledge and then you can fine-tune it for this task, and it hopefully will have a better start. And so, what I'm talking about is making a foundation model for stars that will be generally useful for problems like this. Uh and there's, you know, uh this has been shown before to work well for uh astrophysics. Um the concept of a foundation model is don't start uh your neural network from scratch when you have a new task. Train a neural network on your new task that has already been trained on a similar task. Uh and so Mike Wansley showed that when or at all, that when he had 100 JWST images of galaxies and he wanted to train a neural network to predict whether is a spiral or not, um that when they started with a neural network that had already seen hundreds of thousands of Galaxy Zoo images, it led to a model that was much higher accuracy than a neural network trained from scratch on the 100 JWST images. That's not too surprising, but it shows that pretraining uh is, you know, a very effective method and to drive that point home he showed that even when you start with a neural network that was trained on real images of cats, dogs, and pianos, um and then fine-tune that network on your JWST images, it was still better than starting with a neural network from random weights. And so it seems like even if, you know, they're not that similar uh then uh it's still better to use a pretrained model."
            },
            {
                "timestamp": "04:30",
                "text": "Uh so, how do we go about doing this for stars? So we wanted a model that can take in all the observations we have of a star and is capable of doing many different tasks. And so we found that transformer neural networks can vary their input size. And so they were the optimal algorithm for our task compared to normal neural networks that have a fixed input size. And so, essentially, uh this allows uh us to create a model that can take in spectra of size 100 or 1,000, it allows us to take in spectra from any instrument, any wavelength range, any resolution, um and we do this by embedding the pixels individually. We embed their fluxes into an embedding space, and their wavelengths. We also embed into the same embedding space according to a positional encoding, which we borrowed from language models. It's essentially how language models know the position of a word in a sentence. We use that, but for wavelength, and so it's continuous. So, you can give it any wavelength, which is very important. And by embedding the wavelength and the flux of a pixel and adding it in the embedding space, and feeding that into the transformer, we suddenly have a transformer that can take in 100 Gaia spectra pixels, thousands of APOGEE spectra pixels or both at the same time. And so you really can give it any observation you have of a star, and then request information about that star. Because it's also very flexible on the output as well, and so you can train it to predict anything you have the training data for. And so it makes a prediction and an uncertainty."
            },
            {
                "timestamp": "06:00",
                "text": "Um Okay. So, uh just to prototype our foundation model, um we train it on hundreds of thousands of APOGEE spectra to predict uh like stellar properties and just basic abundances, like oxygen, magnesium, and iron abundance. And just for testing purposes, uh we train it on the full wavelength range of APOGEE for each star. But the first half we give it real APOGEE data, and the second half we give it only synthetic data. So, synthetic spectra. Um and this leaves uh the second half available for testing, which you'll see later. But essentially, we train it"
            },
            {
                "timestamp": "07:00",
                "text": "Pre-train on 100,000 stars to do this task and it leads to good performance. Not very surprising."
            },
            {
                "timestamp": "07:30",
                "text": "Fine-tuning on [Fe/H]. Real APOGEE spectra. Only 100 metal-rich stars."
            },
            {
                "timestamp": "08:00",
                "text": "But now is the fun part. Now, we can see what happens when we take this pre-trained model and we fine-tune it on our motivating problem that I was talking about earlier."
            },
            {
                "timestamp": "08:30",
                "text": "And so, before we get into fine-tuning, we just take the model that was pre-trained on synthetic spectra in this range. And we give it a window of real spectra around two iron lines. So it only saw synthetic spectra in the pre-training step in these wavelengths. And now, we're saying, predict iron abundance from this window of real spectra and it does, you know, all right because it did this really well with synthetic spectra, but real spectra is different. It's not the same as synthetic and so there's like an offset, and it doesn't do well for a lot of stars."
            },
            {
                "timestamp": "09:00",
                "text": "But what's really surprising is that when we fine-tune it on real spectra on a 100, you know, metal-rich stars, it does well, even in the metal-poor range on real spectra and it."
            },
            {
                "timestamp": "09:30",
                "text": "And so, what's happening here? Well, it was able to do this task perfectly with synthetic spectra."
            },
            {
                "timestamp": "09:59",
                "text": "All it needs to do is change its weights just the tiniest bit to work with real spectra instead of synthetic. And it generalizes its knowledge down to the metal-poor range which is really cool, and we we freeze part of the model to ensure it's only changing the weights a little bit."
            },
            {
                "timestamp": "10:30",
                "text": "And so, it is obviously much better to start with our pre-trained model, then to start with a neural network trained from scratch for this task."
            },
            {
                "timestamp": "11:00",
                "text": "But we want this pre-trained model to be able to do many different downstream tasks. Anything that a user might want to fine-tune it on. And so, what if we try a task that wasn't in pre-training, like predicting silicon, titanium, and nickel abundances which are really difficult. Especially, titanium is a difficult abundance to measure. So, it never saw these tasks during pre-training."
            },
            {
                "timestamp": "11:30",
                "text": "But the important part is is that when we train on this, it performs better on these tasks, than a neural network trained from scratch, generally better. And so, it must be transferring some of its knowledge from seeing, you know, hundreds of thousands of stars to predict iron, oxygen, and magnesium to this new task."
            },
            {
                "timestamp": "12:00",
                "text": "But this also emphasizes that we really want the pre-training to be completely comprehensive. It should contain as many tasks as possible, so that the model will be good at any new task that someone might want to fine-tune it for."
            },
            {
                "timestamp": "12:30",
                "text": "And that's what we're doing, going forward. Is we want to scale up and build a foundation model for stellar astrophysics. And so, we want to pre-train it on as many datasets as we can."
            },
            {
                "timestamp": "13:00",
                "text": "And so, we can give it, you know, synthetic spectra of all wavelengths. We can give it telescopic, like observations from many different surveys. We can even give it non-spectra inputs. That's how flexible this model is. So, the same neural network, we can give it photometry, you know, reddening, radial velocities, and have it do all these different tasks from all these different surveys and instruments so that we build a foundation model that can generally do stellar astrophysics well. And it's going to be a much better starting point when you fine-tune it on your own task, then a neural network trained from scratch."
            },
            {
                "timestamp": "13:30",
                "text": "That's all. Thank you very much."
            },
            {
                "timestamp": "13:59",
                "text": "Thank you for a nice talk. Questions over there?"
            },
            {
                "timestamp": "14:00",
                "text": "and hopefully it learns how to do this with different instruments."
            },
            {
                "timestamp": "14:00",
                "text": "Yeah, there's a question from online asking, can you explain how you got your synthetic spectrum?"
            },
            {
                "timestamp": "14:30",
                "text": "If not then yeah we're going to try and figure out what it needs to know in order to work with different instruments or different systematics."
            },
            {
                "timestamp": "14:30",
                "text": "Yeah Thanks."
            },
            {
                "timestamp": "14:30",
                "text": "Thank you We have time maybe before one."
            },
            {
                "timestamp": "14:30",
                "text": "So it's really really great work."
            },
            {
                "timestamp": "14:30",
                "text": "Yeah Thanks I was wondering if you played around with different parameter accounts for the model and if you've found similar scaling rules that we seen in the texture models."
            },
            {
                "timestamp": "14:30",
                "text": "Yeah that is definitely only a to do this thing is we really want to see this beautiful scaling law that I think you saw."
            },
            {
                "timestamp": "14:30",
                "text": "And that if you like increase the number of parameters or the amount of data then you see a nice scaling law with the train."
            },
            {
                "timestamp": "14:30",
                "text": "Or with the test loss or accuracy."
            },
            {
                "timestamp": "14:30",
                "text": "And no I have not done it so far."
            },
            {
                "timestamp": "14:30",
                "text": "I've just been limited by compute and how long."
            },
            {
                "timestamp": "14:30",
                "text": "How long I want to you know wait for the training."
            },
            {
                "timestamp": "14:30",
                "text": "Right now it's fairly small it's like 5 million I think the parameters are small."
            },
            {
                "timestamp": "14:30",
                "text": "But we can definitely scale it up because yeah we have some A100 times."
            },
            {
                "timestamp": "14:30",
                "text": "So we just the pre-training took like maybe a day two days about and then fine-tuning takes like an hour."
            },
            {
                "timestamp": "14:30",
                "text": "Yeah Thanks."
            },
            {
                "timestamp": "14:30",
                "text": "One more question from online Could it be helpful if you feed spectra."
            },
            {
                "timestamp": "14:30",
                "text": "Yeah with other mocks."
            },
            {
                "timestamp": "14:30",
                "text": "What do you mean by other mocks like other synthetic spectra generation."
            },
            {
                "timestamp": "14:30",
                "text": "Yeah I mean this is a wonderful part is that it's so flexible that you can just define a new input and give it that input and so."
            },
            {
                "timestamp": "14:30",
                "text": "Yeah we haven't really decided what synthetic spectra we're going to give it in the long run but yeah it's totally possible to give it."
            },
            {
                "timestamp": "14:30",
                "text": "Synthetic spectra from different pipelines and that's going to require some experiment to see how it handles that or is if that's even useful."
            },
            {
                "timestamp": "14:30",
                "text": "Yeah Thanks Alright So."
            },
            {
                "timestamp": "14:30",
                "text": "Thank you so much Let's stop now again we need to move on to the next talk apologies for those who have questions we can have them in the in the breaks."
            },
            {
                "timestamp": "14:30",
                "text": "And the the next p p p p p p p p p p p"
            }
        ]
    }
]